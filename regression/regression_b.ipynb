{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg as linalg\n",
    "from dtuimldmtools import similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import svd\n",
    "from pandas.plotting import scatter_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.linear_model as lm\n",
    "from matplotlib.pyplot import figure, legend, plot, show, xlabel, ylabel\n",
    "# exercise 8.1.1\n",
    "import torch\n",
    "import importlib_resources\n",
    "import numpy as np\n",
    "import sklearn.linear_model as lm\n",
    "from matplotlib.pylab import (\n",
    "    figure,\n",
    "    grid,\n",
    "    legend,\n",
    "    loglog,\n",
    "    semilogx,\n",
    "    show,\n",
    "    subplot,\n",
    "    title,\n",
    "    xlabel,\n",
    "    ylabel,\n",
    ")\n",
    "from scipy.io import loadmat\n",
    "from sklearn import model_selection\n",
    "import importlib_resources\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy import stats\n",
    "from scipy.io import loadmat\n",
    "from sklearn import model_selection\n",
    "from dtuimldmtools import draw_neural_net, train_neural_net\n",
    "\n",
    "\n",
    "from dtuimldmtools import rlr_validate\n",
    "\n",
    "# fetch dataset \n",
    "# wine = fetch_ucirepo(id=109) \n",
    "  \n",
    "# # data (as pandas dataframes) \n",
    "# X = wine.data.features \n",
    "# y = wine.data.targets \n",
    "\n",
    "# totaldata= (wine.data)\n",
    "  \n",
    "# metadata \n",
    "# print(wine.metadata) \n",
    "  \n",
    "# variable information \n",
    "# print(wine.variables) \n",
    "\n",
    "# OFFLINE LOADING OF DATA\n",
    "X = np.loadtxt('../wine/wine.data', delimiter=',')\n",
    "y = X[:,0]\n",
    "X = np.delete(X,0,axis=1)\n",
    "# y = np.loadtxt('./wine/wine.names', delimiter=',')\n",
    "\n",
    "Xorig = X\n",
    "# Standardizing the data\n",
    "\n",
    "X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "\n",
    "# One of K encoding\n",
    "# y = similarity.one_of_k(y)\n",
    "\n",
    "# Ensure y values are within the range of the identity matrix's size\n",
    "num_classes = int(np.max(y)) + 1\n",
    "y2 = np.eye(num_classes)[y.astype(int)]\n",
    "\n",
    "#removing first column of y\n",
    "y2 = y2[:,1:]\n",
    "\n",
    "#Appending y to X\n",
    "X = np.append(X,y2,axis=1)\n",
    "\n",
    "#Extracting the first column of X\n",
    "y = X[:,0]\n",
    "\n",
    "\n",
    "\n",
    "attributeNames = [\n",
    "    \"Alcohol\",\n",
    "    \"Malic acid\",\n",
    "    \"Ash\",\n",
    "    \"Alcalinity of ash\",\n",
    "    \"Magnesium\",\n",
    "    \"Total phenols\",\n",
    "    \"Flavanoids\",\n",
    "    \"Nonflavanoid phenols\",\n",
    "    \"Proanthocyanins\",\n",
    "    \"Color intensity\",\n",
    "    \"Hue\",\n",
    "    \"OD280/OD315 of diluted wines\",\n",
    "    \"Proline\"\n",
    "]\n",
    "\n",
    "#Removing the first column of X\n",
    "X = np.delete(X,0,axis=1)\n",
    "# Removing alcohol from attributenames\n",
    "attributeNames = attributeNames[1:]\n",
    "N, M = X.shape\n",
    "\n",
    "# Add offset attribute\n",
    "X = np.concatenate((np.ones((X.shape[0], 1)), X), 1)\n",
    "attributeNames = [\"Offset\"] + attributeNames + [\"Class 1\", \"Class 2\", \"Class 3\"]\n",
    "M = M + 1\n",
    "\n",
    "## Crossvalidation\n",
    "# Create crossvalidation partition for evaluation\n",
    "K = 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Crossvalidation fold: 1/10\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0653058\t5.0355407e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1622\t1.0637062\t8.9655725e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0642213\t3.4724687e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1277\t1.0636415\t8.966117e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0683911\t1.2942918e-05\n",
      "\t\t2000\t1.0636047\t1.2328834e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2000\t1.0636047\t1.2328834e-06\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0646794\t5.598334e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1542\t1.0631195\t8.9705196e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0494462\t2.839806e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1270\t1.0489432\t9.091755e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0514016\t8.957043e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1636\t1.0488038\t9.092963e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t972\t1.0486699\t9.094124e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0540192\t9.952677e-06\n",
      "\t\t2000\t1.0493871\t1.7039816e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2000\t1.0493871\t1.7039816e-06\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0338537\t1.3836673e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1054\t1.0337822\t9.22509e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0339233\t1.4988718e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1069\t1.0338348\t9.224621e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t953\t1.0341189\t8.069327e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0346129\t2.4196383e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1227\t1.0342172\t9.22121e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0774678\t3.6510532e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1390\t1.0765724\t9.965726e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0764272\t1.3289422e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1013\t1.0764107\t9.967223e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0856732\t2.0642385e-05\n",
      "\t\t2000\t1.0768372\t1.7712475e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2000\t1.0768372\t1.7712475e-06\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0841092\t1.7593386e-05\n",
      "\t\t2000\t1.0771357\t1.6600846e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2000\t1.0771357\t1.6600846e-06\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0618895\t1.9196345e-05\n",
      "\t\t2000\t1.0553129\t2.5980983e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2000\t1.0553129\t2.5980983e-06\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t685\t1.0533836\t9.05343e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0542837\t2.148351e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1282\t1.0538201\t9.04968e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0582649\t8.673665e-06\n",
      "\t\t2000\t1.0543333\t1.4698563e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2000\t1.0543333\t1.4698563e-06\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0719857\t4.1145377e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1597\t1.0705838\t8.907976e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0704086\t1.6705176e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1155\t1.070183\t8.9113115e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0702643\t1.3365947e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1054\t1.0701947\t8.9112143e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0754446\t1.0751997e-05\n",
      "\t\t2000\t1.0711371\t1.6693815e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2000\t1.0711371\t1.6693815e-06\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0250976\t1.0814915e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1424\t1.0231425\t9.321023e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0262942\t6.9692564e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1882\t1.023334\t9.319278e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0247927\t4.6529894e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1524\t1.0235205\t9.3175805e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.028863\t1.0196023e-05\n",
      "\t\t2000\t1.0246575\t1.7451063e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2000\t1.0246575\t1.7451063e-06\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.98012835\t1.0946238e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1810\t0.9771805\t8.5395106e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.97665256\t1.4036772e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1027\t0.97661865\t9.765054e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.97860926\t6.943407e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1548\t0.9769537\t9.761706e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.9836577\t1.1028151e-05\n",
      "\t\t2000\t0.97810644\t2.4984852e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2000\t0.97810644\t2.4984852e-06\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.1072645\t2.6806889e-05\n",
      "\t\t2000\t1.0966791\t3.0435976e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2000\t1.0966791\t3.0435976e-06\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0978603\t8.035099e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1876\t1.0946797\t9.800881e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0959048\t6.5265804e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1442\t1.0943713\t9.803642e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0976857\t5.9729955e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1885\t1.0949484\t8.709757e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.02094\t1.3894753e-05\n",
      "\t\t2000\t1.0152836\t1.8786328e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2000\t1.0152836\t1.8786328e-06\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.022733\t1.7949847e-05\n",
      "\t\t2000\t1.0152117\t2.4658787e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2000\t1.0152117\t2.4658787e-06\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0172311\t8.906359e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1688\t1.0147204\t9.3983857e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0157834\t3.4033412e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1468\t1.0148152\t9.397508e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0183678\t9.59876e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1843\t1.0151118\t9.3947625e-07\n",
      "\n",
      "Crossvalidation fold: 2/10\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lauge/Desktop/ML/MLenv/lib/python3.12/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([144])) that is different to the input size (torch.Size([144, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t1000\t1.0641013\t2.12853e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1290\t1.0636231\t8.966273e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lauge/Desktop/ML/MLenv/lib/python3.12/site-packages/dtuimldmtools/models/nn_trainer.py:141: RuntimeWarning: overflow encountered in cast\n",
      "  if loss_value < best_final_loss:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tFinal loss:\n",
      "\t\t853\t1.0632325\t8.9695664e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0659487\t5.2561695e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1891\t1.0633768\t8.9683493e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0650107\t3.4698949e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1653\t1.0637133\t8.965512e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t394\t1.0483656\t9.0967643e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0496778\t4.996945e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1446\t1.0485506\t9.095159e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0519775\t8.272236e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1903\t1.0487355\t9.0935555e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0515089\t8.502658e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1759\t1.0490712\t9.0906457e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0358198\t5.178884e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1577\t1.0342908\t9.2205545e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0344387\t1.4981249e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1113\t1.034271\t9.2207307e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.039445\t9.97754e-06\n",
      "\t\t2000\t1.0351007\t1.6123333e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2000\t1.0351007\t1.6123333e-06\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0344373\t2.0743284e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1145\t1.0342082\t9.2212906e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t852\t1.0760689\t9.970389e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0812728\t1.1686261e-05\n",
      "\t\t2000\t1.0766746\t1.4393564e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2000\t1.0766746\t1.4393564e-06\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0778049\t5.0877475e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1487\t1.0764624\t9.966744e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0794142\t7.8410985e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1740\t1.0768833\t9.962848e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.056774\t1.2069979e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1730\t1.0535268\t9.0521996e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.056247\t8.01308e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1712\t1.0536141\t9.051449e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.064116\t1.7139775e-05\n",
      "\t\t2000\t1.0548627\t2.938233e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2000\t1.0548627\t2.938233e-06\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.055764\t5.871432e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1674\t1.0537062\t9.0506586e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.070298\t1.8934485e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1167\t1.0700492\t8.912427e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.074161\t7.3245587e-06\n",
      "\t\t2000\t1.0705407\t1.5589576e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2000\t1.0705407\t1.5589576e-06\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0708791\t4.0074715e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1363\t1.0700084\t8.912766e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0704231\t1.3363964e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1061\t1.0703473\t8.909944e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.023319\t1.3979115e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1031\t1.0232811\t9.31976e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0235363\t1.7470178e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1144\t1.0233345\t9.319274e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0264834\t7.664764e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1828\t1.0235733\t9.3170996e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.025139\t5.465411e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1526\t1.0237455\t9.315532e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.98113817\t2.1687461e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1854\t0.97721314\t9.759113e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.9851807\t2.4986384e-05\n",
      "\t\t2000\t0.97710806\t1.5250246e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2000\t0.97710806\t1.5250246e-06\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.98121274\t1.2027543e-05\n",
      "\t\t2000\t0.9773047\t1.158786e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2000\t0.9773047\t1.158786e-06\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.9777456\t2.804212e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1419\t0.9770112\t9.76113e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0942327\t1.6341464e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1072\t1.0941205\t9.80589e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t490\t1.0940908\t9.806156e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t739\t1.0943321\t9.803994e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.1017859\t1.5363656e-05\n",
      "\t\t2000\t1.0954505\t1.958795e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2000\t1.0954505\t1.958795e-06\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t979\t1.0146409\t9.3991224e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0147941\t4.698834e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1193\t1.0142883\t9.40239e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0144742\t1.6451156e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1087\t1.0143534\t9.4017867e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0151799\t2.2311035e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1280\t1.0147501\t9.398111e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0150046\t2.4663818e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1312\t1.0144628\t9.4007726e-07\n",
      "\n",
      "Crossvalidation fold: 3/10\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lauge/Desktop/ML/MLenv/lib/python3.12/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([144])) that is different to the input size (torch.Size([144, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t1000\t1.0646489\t5.710464e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1522\t1.0631514\t8.9702513e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lauge/Desktop/ML/MLenv/lib/python3.12/site-packages/dtuimldmtools/models/nn_trainer.py:141: RuntimeWarning: overflow encountered in cast\n",
      "  if loss_value < best_final_loss:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t1000\t1.0633078\t2.4664523e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1121\t1.0630844\t8.9708163e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.06527\t6.4904616e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1607\t1.0634758\t8.9675143e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0637338\t1.6809998e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1175\t1.0634909\t8.9673875e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.050895\t7.146415e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1635\t1.0488176\t7.9562386e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0494025\t5.452641e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1285\t1.0486026\t9.094708e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0489963\t2.0455388e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1197\t1.0486797\t9.094039e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0520821\t1.1104058e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1773\t1.0490181\t9.0911055e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0366142\t4.7149247e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1716\t1.0349398\t9.2147724e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t665\t1.034139\t9.2219074e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0358229\t5.2939536e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1625\t1.0340444\t9.2227515e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0363631\t7.93677e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1625\t1.0341588\t9.221731e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t931\t1.0759338\t9.971641e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0778313\t4.7558233e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1520\t1.0764416\t9.966936e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0762298\t1.329186e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1033\t1.0761855\t9.969309e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t958\t1.0762043\t9.969134e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0540301\t2.2619663e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1223\t1.0536484\t9.051155e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0612607\t2.0667925e-05\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Generalization error\n",
    "#two-fold K=10 fold cross validation \n",
    "# K = 10\n",
    "# N = X.shape[0]\n",
    "# M = X.shape[1]\n",
    "# #splitting the data into K folds\n",
    "# fold_size = N//K\n",
    "# np.random.shuffle(X)\n",
    "# folds = np.split(X, K)\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "CV = model_selection.KFold(K, shuffle=True)\n",
    "\n",
    "# Creating a outer cross-validation\n",
    "# Outer cross-validation\n",
    "nested_cv = model_selection.KFold(n_splits=K, shuffle=True, random_state=1)\n",
    "\n",
    "#CV = model_selection.KFold(K, shuffle=False)\n",
    "\n",
    "# Values of lambda\n",
    "lambdas = np.power(10.0, range(-4, 10))\n",
    "\n",
    "# Initialize variables\n",
    "# T = len(lambdas)\n",
    "Error_train = np.empty((K, 1))\n",
    "Error_test = np.empty((K, 1))\n",
    "Error_train_rlr = np.empty((K, 1))\n",
    "Error_test_rlr = np.empty((K, 1))\n",
    "Error_train_nofeatures = np.empty((K, 1))\n",
    "Error_test_nofeatures = np.empty((K, 1))\n",
    "w_rlr = np.empty((M, K))\n",
    "mu = np.empty((K, M - 1))\n",
    "sigma = np.empty((K, M - 1))\n",
    "w_noreg = np.empty((M, K))\n",
    "best_hidden_units = []\n",
    "best_lambdas = []\n",
    "\n",
    "# Initialize arrays to store errors for each fold\n",
    "baseline_errors_train = np.empty(K)\n",
    "baseline_errors_test = np.empty(K)\n",
    "\n",
    "outer_errors = []\n",
    "# For ANN\n",
    "# Parameters for neural network classifier\n",
    "outer_best_hidden_unit =[]\n",
    "\n",
    "\n",
    "# Setup figure for display of learning curves and error rates in fold\n",
    "summaries, summaries_axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "# Make a list for storing assigned color of learning curve for up to K=10\n",
    "color_list = [\n",
    "    \"tab:orange\",\n",
    "    \"tab:green\",\n",
    "    \"tab:purple\",\n",
    "    \"tab:brown\",\n",
    "    \"tab:pink\",\n",
    "    \"tab:gray\",\n",
    "    \"tab:olive\",\n",
    "    \"tab:cyan\",\n",
    "    \"tab:red\",\n",
    "    \"tab:blue\",\n",
    "]\n",
    "# Define the model\n",
    "# model = lambda: torch.nn.Sequential(\n",
    "#     torch.nn.Linear(M, n_hidden_units),  # M features to n_hidden_units\n",
    "#     torch.nn.Tanh(),  # 1st transfer function,\n",
    "#     torch.nn.Linear(n_hidden_units, 1),  # n_hidden_units to 1 output neuron\n",
    "#     # no final tranfer function, i.e. \"linear output\"\n",
    "# )\n",
    "# loss_fn = torch.nn.MSELoss()  # notice how this is now a mean-squared-error loss\n",
    "\n",
    "# print(\"Training model of type:\\n\\n{}\\n\".format(str(model())))\n",
    "errors = []  # make a list for storing generalizaition error in each loop\n",
    "\n",
    "internal_cross_validation = 10\n",
    "\n",
    "internal_cv = model_selection.KFold(internal_cross_validation, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "k = 0\n",
    "\n",
    "for train_index, test_index in CV.split(X, y):\n",
    "    # extract training and test set for current CV fold\n",
    "    X_train = X[train_index]\n",
    "    y_train = y[train_index]\n",
    "    X_test = X[test_index]\n",
    "    y_test = y[test_index]\n",
    "    internal_cross_validation = 10\n",
    "\n",
    "    (\n",
    "        opt_val_err,\n",
    "        opt_lambda,\n",
    "        mean_w_vs_lambda,\n",
    "        train_err_vs_lambda,\n",
    "        test_err_vs_lambda,\n",
    "    ) = rlr_validate(X_train, y_train, lambdas, internal_cross_validation)\n",
    "\n",
    "    # Standardize outer fold based on training set, and save the mean and standard\n",
    "    # deviations since they're part of the model (they would be needed for\n",
    "    # making new predictions) - for brevity we won't always store these in the scripts\n",
    "    mu[k, :] = np.mean(X_train[:, 1:], 0)\n",
    "    sigma[k, :] = np.std(X_train[:, 1:], 0)\n",
    "\n",
    "    X_train[:, 1:] = (X_train[:, 1:] - mu[k, :]) / sigma[k, :]\n",
    "    X_test[:, 1:] = (X_test[:, 1:] - mu[k, :]) / sigma[k, :]\n",
    "\n",
    "    Xty = X_train.T @ y_train\n",
    "    XtX = X_train.T @ X_train\n",
    "\n",
    "    # Compute mean squared error without using the input data at all\n",
    "    Error_train_nofeatures[k] = (\n",
    "        np.square(y_train - y_train.mean()).sum(axis=0) / y_train.shape[0]\n",
    "    )\n",
    "    Error_test_nofeatures[k] = (\n",
    "        np.square(y_test - y_test.mean()).sum(axis=0) / y_test.shape[0]\n",
    "    )\n",
    "\n",
    "    # Estimate weights for the optimal value of lambda, on entire training set\n",
    "    lambdaI = opt_lambda * np.eye(M)\n",
    "    lambdaI[0, 0] = 0  # Do no regularize the bias term\n",
    "    w_rlr[:, k] = np.linalg.solve(XtX + lambdaI, Xty).squeeze()\n",
    "    # Compute mean squared error with regularization with optimal lambda\n",
    "    Error_train_rlr[k] = (\n",
    "        np.square(y_train - X_train @ w_rlr[:, k]).sum(axis=0) / y_train.shape[0]\n",
    "    )\n",
    "    Error_test_rlr[k] = (\n",
    "        np.square(y_test - X_test @ w_rlr[:, k]).sum(axis=0) / y_test.shape[0]\n",
    "    )\n",
    "\n",
    "    # Estimate weights for unregularized linear regression, on entire training set\n",
    "    w_noreg[:, k] = np.linalg.solve(XtX, Xty).squeeze()\n",
    "    # Compute mean squared error without regularization\n",
    "    Error_train[k] = (\n",
    "        np.square(y_train - X_train @ w_noreg[:, k]).sum(axis=0) / y_train.shape[0]\n",
    "    )\n",
    "    Error_test[k] = (\n",
    "        np.square(y_test - X_test @ w_noreg[:, k]).sum(axis=0) / y_test.shape[0]\n",
    "    )\n",
    "    # OR ALTERNATIVELY: you can use sklearn.linear_model module for linear regression:\n",
    "    # m = lm.LinearRegression().fit(X_train, y_train)\n",
    "    # Error_train[k] = np.square(y_train-m.predict(X_train)).sum()/y_train.shape[0]\n",
    "    # Error_test[k] = np.square(y_test-m.predict(X_test)).sum()/y_test.shape[0]\n",
    "\n",
    "    best_lambdas.append(opt_lambda)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #_____________________________________________________________________________\n",
    "    \n",
    "    # Now we want to make a ANN to model the data\n",
    "    max_hidden_units = 5  # number of hidden units\n",
    "    n_replicates = 1  # number of networks trained in each k-fold\n",
    "    max_iter = 2000\n",
    "    print(\"\\nCrossvalidation fold: {0}/{1}\".format(k + 1, K))\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    for train_inner_idx, val_inner_idx in internal_cv.split(X_train, y_train):\n",
    "        # Extract training and test set for current CV fold, convert to tensors\n",
    "        X_train_inner = torch.Tensor(X[train_inner_idx, :])\n",
    "        y_train_inner = torch.Tensor(y[train_inner_idx])\n",
    "        X_test_inner = torch.Tensor(X[val_inner_idx, :])\n",
    "        y_test_inner = torch.Tensor(y[val_inner_idx])\n",
    "        error_log = []  # make a list for storing generalizaition error in each loop\n",
    "        for n_hidden_units in range(1, max_hidden_units):\n",
    "            \n",
    "            model = lambda: torch.nn.Sequential(\n",
    "                torch.nn.Linear(M, n_hidden_units),  \n",
    "                torch.nn.Tanh(),    \n",
    "                torch.nn.Linear(n_hidden_units, 1)   \n",
    "            )\n",
    "            \n",
    "            net, _, _ = train_neural_net(\n",
    "                model,\n",
    "                loss_fn,\n",
    "                X=X_train_inner,\n",
    "                y=y_train_inner,\n",
    "                n_replicates=n_replicates,\n",
    "                max_iter=max_iter,\n",
    "            )\n",
    "\n",
    "            # print(\"\\n\\tBest loss: {}\\n\".format(final_loss))\n",
    "\n",
    "            # Determine estimated value for test set\n",
    "            y_test_inner_est = net(X_test_inner)\n",
    "\n",
    "            # Determine errors\n",
    "            se = (y_test_inner_est.float() - y_test_inner.float()) ** 2  # squared error\n",
    "            mse = (sum(se).type(torch.float) / len(y_test_inner)).data.numpy()  # mean\n",
    "\n",
    "            error_log.append(np.mean(mse))\n",
    "        # Determine errors for every hidden unit\n",
    "        # store error rate for current CV fold\n",
    "        best_hidden_unit = range(1, max_hidden_units)[np.argmin(error_log)]\n",
    "        best_hidden_units.append(best_hidden_unit)\n",
    "        errors.append((error_log[np.argmin(error_log)]))  \n",
    "        \n",
    "        \n",
    "        \n",
    "    # Outer ANN\n",
    "    #_____________________________________________________________________________\n",
    "    X_train = torch.Tensor(X[train_index, :])\n",
    "    y_train = torch.Tensor(y[train_index])\n",
    "    X_test = torch.Tensor(X[test_index, :])\n",
    "    y_test = torch.Tensor(y[test_index])\n",
    "    best_hidden_unit_inner = best_hidden_units[np.argmin(errors)]    \n",
    "    outer_best_hidden_unit.append(best_hidden_units[np.argmin(errors)])  #The best hidden unit for the outer loop\n",
    "    loss_fn = torch.nn.MSELoss()    \n",
    "    model = lambda: torch.nn.Sequential(\n",
    "                torch.nn.Linear(M, n_hidden_units),  \n",
    "                torch.nn.Tanh(),    \n",
    "                torch.nn.Linear(n_hidden_units, 1)   \n",
    "            )\n",
    "    # Train the net on training data\n",
    "    net, final_loss, learning_curve = train_neural_net(\n",
    "                model,\n",
    "                loss_fn,\n",
    "                X=X_train,\n",
    "                y=y_train,\n",
    "                n_replicates=n_replicates,\n",
    "                max_iter=max_iter,\n",
    "    )\n",
    "\n",
    "    # print(\"\\n\\tBest loss: {}\\n\".format(final_loss))\n",
    "\n",
    "    # Determine estimated value for test set\n",
    "    y_test_est = net(X_test)\n",
    "\n",
    "    # Determine errors\n",
    "    se = (y_test_est.float() - y_test.float()) ** 2  # squared error\n",
    "    mse = (sum(se).type(torch.float) / len(y_test)).data.numpy()  # mean\n",
    "    outer_errors.append(mse)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # BASELINE MODEL\n",
    "    #_____________________________________________________________________________\n",
    "    \n",
    "    X_train, y_train = X[train_index, :], y[train_index]\n",
    "    X_test, y_test = X[test_index, :], y[test_index]\n",
    "\n",
    "    # Step 3: Baseline model - predict the mean of y_train\n",
    "    mean_y_train = np.mean(y_train)\n",
    "\n",
    "    # Predict the mean for both training and test sets\n",
    "    y_pred_train = np.full(len(y_train), mean_y_train)\n",
    "    y_pred_test = np.full(len(y_test), mean_y_train)\n",
    "\n",
    "    # Step 4: Calculate Mean Squared Error (MSE) for training and test sets\n",
    "    mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "    mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "\n",
    "    # Store the errors\n",
    "    baseline_errors_train[k] = mse_train\n",
    "    baseline_errors_test[k] = mse_test\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    k += 1\n",
    "    \n",
    "    \n",
    "    \n",
    "# THE ERRORS FOR THE LAMBDAS ARE:\n",
    "print(\"The errors for the regularized logistic regression is:\" ,Error_train_rlr)\n",
    "print(\"and the best lambdas are: \", best_lambdas)\n",
    "# THE MSE FOR THE ANN ARE:\n",
    "print(\"outer best hidden units\",outer_best_hidden_unit)\n",
    "print(\"outer errors\",outer_errors)\n",
    "meanErrors = [np.mean(error) for error in errors]\n",
    "print(\"The errors for the ANN are:\" ,meanErrors)\n",
    "print(\"and the best hidden units are: \", best_hidden_units)\n",
    "# Take the mean of every array in errors\n",
    "\n",
    "#The MSE FOR THE BASELINE\n",
    "print(\"The errors for the baseline are:\" ,baseline_errors_test)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
