{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg as linalg\n",
    "from dtuimldmtools import similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import svd\n",
    "from pandas.plotting import scatter_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.linear_model as lm\n",
    "from matplotlib.pyplot import figure, legend, plot, show, xlabel, ylabel\n",
    "# exercise 8.1.1\n",
    "import torch\n",
    "import importlib_resources\n",
    "import numpy as np\n",
    "import sklearn.linear_model as lm\n",
    "from matplotlib.pylab import (\n",
    "    figure,\n",
    "    grid,\n",
    "    legend,\n",
    "    loglog,\n",
    "    semilogx,\n",
    "    show,\n",
    "    subplot,\n",
    "    title,\n",
    "    xlabel,\n",
    "    ylabel,\n",
    ")\n",
    "from scipy.io import loadmat\n",
    "from sklearn import model_selection\n",
    "import importlib_resources\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy import stats\n",
    "from scipy.io import loadmat\n",
    "from sklearn import model_selection\n",
    "from dtuimldmtools import draw_neural_net, train_neural_net\n",
    "\n",
    "\n",
    "from dtuimldmtools import rlr_validate\n",
    "\n",
    "# fetch dataset \n",
    "# wine = fetch_ucirepo(id=109) \n",
    "  \n",
    "# # data (as pandas dataframes) \n",
    "# X = wine.data.features \n",
    "# y = wine.data.targets \n",
    "\n",
    "# totaldata= (wine.data)\n",
    "  \n",
    "# metadata \n",
    "# print(wine.metadata) \n",
    "  \n",
    "# variable information \n",
    "# print(wine.variables) \n",
    "\n",
    "# OFFLINE LOADING OF DATA\n",
    "X = np.loadtxt('../wine/wine.data', delimiter=',')\n",
    "y = X[:,0]\n",
    "X = np.delete(X,0,axis=1)\n",
    "# y = np.loadtxt('./wine/wine.names', delimiter=',')\n",
    "\n",
    "Xorig = X\n",
    "# Standardizing the data\n",
    "\n",
    "X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "\n",
    "# One of K encoding\n",
    "# y = similarity.one_of_k(y)\n",
    "\n",
    "# Ensure y values are within the range of the identity matrix's size\n",
    "num_classes = int(np.max(y)) + 1\n",
    "y2 = np.eye(num_classes)[y.astype(int)]\n",
    "\n",
    "#removing first column of y\n",
    "y2 = y2[:,1:]\n",
    "\n",
    "#Appending y to X\n",
    "X = np.append(X,y2,axis=1)\n",
    "\n",
    "#Extracting the first column of X\n",
    "y = X[:,0]\n",
    "\n",
    "\n",
    "\n",
    "attributeNames = [\n",
    "    \"Alcohol\",\n",
    "    \"Malic acid\",\n",
    "    \"Ash\",\n",
    "    \"Alcalinity of ash\",\n",
    "    \"Magnesium\",\n",
    "    \"Total phenols\",\n",
    "    \"Flavanoids\",\n",
    "    \"Nonflavanoid phenols\",\n",
    "    \"Proanthocyanins\",\n",
    "    \"Color intensity\",\n",
    "    \"Hue\",\n",
    "    \"OD280/OD315 of diluted wines\",\n",
    "    \"Proline\"\n",
    "]\n",
    "\n",
    "#Removing the first column of X\n",
    "X = np.delete(X,0,axis=1)\n",
    "# Removing alcohol from attributenames\n",
    "attributeNames = attributeNames[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model of type:\n",
      "\n",
      "Sequential(\n",
      "  (0): Linear(in_features=19, out_features=2, bias=True)\n",
      "  (1): Tanh()\n",
      "  (2): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n",
      "\n",
      "\n",
      "Crossvalidation fold: 1/10\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dy/wj37svsd49x4sv850g87mfym0000gn/T/ipykernel_63714/3202068233.py:106: RuntimeWarning: invalid value encountered in divide\n",
      "  X_train[:, 1:] = (X_train[:, 1:] - mu[k, :]) / sigma[k, :]\n",
      "/var/folders/dy/wj37svsd49x4sv850g87mfym0000gn/T/ipykernel_63714/3202068233.py:107: RuntimeWarning: invalid value encountered in divide\n",
      "  X_test[:, 1:] = (X_test[:, 1:] - mu[k, :]) / sigma[k, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t1000\t1.0176541\t6.442728e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1772\t1.0155272\t9.3909193e-07\n",
      "\n",
      "\tBest loss: 1.0155272483825684\n",
      "\n",
      "\n",
      "Crossvalidation fold: 2/10\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0091195\t1.2994501e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1001\t1.0091186\t9.450559e-07\n",
      "\n",
      "\tBest loss: 1.0091185569763184\n",
      "\n",
      "\n",
      "Crossvalidation fold: 3/10\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.9954235\t7.963801e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1956\t0.9923002\t9.610735e-07\n",
      "\n",
      "\tBest loss: 0.9923002123832703\n",
      "\n",
      "\n",
      "Crossvalidation fold: 4/10\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.9878455\t2.5341906e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1315\t0.9873092\t9.659318e-07\n",
      "\n",
      "\tBest loss: 0.9873092174530029\n",
      "\n",
      "\n",
      "Crossvalidation fold: 5/10\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0097275\t2.8334525e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1238\t1.0092802\t9.449045e-07\n",
      "\n",
      "\tBest loss: 1.0092802047729492\n",
      "\n",
      "\n",
      "Crossvalidation fold: 6/10\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t959\t1.0158601\t9.3878424e-07\n",
      "\n",
      "\tBest loss: 1.0158600807189941\n",
      "\n",
      "\n",
      "Crossvalidation fold: 7/10\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.9496628\t1.3180425e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1052\t0.9496026\t9.415189e-07\n",
      "\n",
      "\tBest loss: 0.9496026039123535\n",
      "\n",
      "\n",
      "Crossvalidation fold: 8/10\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0329562\t1.4194729e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1744\t1.0292546\t9.265671e-07\n",
      "\n",
      "\tBest loss: 1.0292545557022095\n",
      "\n",
      "\n",
      "Crossvalidation fold: 9/10\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.9703229\t1.7199709e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1140\t0.9701311\t9.830355e-07\n",
      "\n",
      "\tBest loss: 0.9701310992240906\n",
      "\n",
      "\n",
      "Crossvalidation fold: 10/10\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.0278659\t9.278112e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1739\t1.0251551\t9.302724e-07\n",
      "\n",
      "\tBest loss: 1.0251550674438477\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Generalization error\n",
    "#two-fold K=10 fold cross validation \n",
    "# K = 10\n",
    "# N = X.shape[0]\n",
    "# M = X.shape[1]\n",
    "# #splitting the data into K folds\n",
    "# fold_size = N//K\n",
    "# np.random.shuffle(X)\n",
    "# folds = np.split(X, K)\n",
    "\n",
    "N, M = X.shape\n",
    "\n",
    "# Add offset attribute\n",
    "X = np.concatenate((np.ones((X.shape[0], 1)), X), 1)\n",
    "attributeNames = [\"Offset\"] + attributeNames + [\"Class 1\", \"Class 2\", \"Class 3\"]\n",
    "M = M + 1\n",
    "\n",
    "## Crossvalidation\n",
    "# Create crossvalidation partition for evaluation\n",
    "K = 10\n",
    "\n",
    "CV = model_selection.KFold(K, shuffle=True)\n",
    "\n",
    "# Creating a nested cross-validation\n",
    "# Nested cross-validation\n",
    "nested_cv = model_selection.KFold(n_splits=K, shuffle=True, random_state=1)\n",
    "\n",
    "#CV = model_selection.KFold(K, shuffle=False)\n",
    "\n",
    "# Values of lambda\n",
    "lambdas = np.power(10.0, range(-4, 10))\n",
    "\n",
    "# Initialize variables\n",
    "# T = len(lambdas)\n",
    "Error_train = np.empty((K, 1))\n",
    "Error_test = np.empty((K, 1))\n",
    "Error_train_rlr = np.empty((K, 1))\n",
    "Error_test_rlr = np.empty((K, 1))\n",
    "Error_train_nofeatures = np.empty((K, 1))\n",
    "Error_test_nofeatures = np.empty((K, 1))\n",
    "w_rlr = np.empty((M, K))\n",
    "mu = np.empty((K, M - 1))\n",
    "sigma = np.empty((K, M - 1))\n",
    "w_noreg = np.empty((M, K))\n",
    "\n",
    "\n",
    "# For ANN\n",
    "# Parameters for neural network classifier\n",
    "n_hidden_units = 2  # number of hidden units\n",
    "n_replicates = 1  # number of networks trained in each k-fold\n",
    "max_iter = 10000\n",
    "\n",
    "\n",
    "# Setup figure for display of learning curves and error rates in fold\n",
    "summaries, summaries_axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "# Make a list for storing assigned color of learning curve for up to K=10\n",
    "color_list = [\n",
    "    \"tab:orange\",\n",
    "    \"tab:green\",\n",
    "    \"tab:purple\",\n",
    "    \"tab:brown\",\n",
    "    \"tab:pink\",\n",
    "    \"tab:gray\",\n",
    "    \"tab:olive\",\n",
    "    \"tab:cyan\",\n",
    "    \"tab:red\",\n",
    "    \"tab:blue\",\n",
    "]\n",
    "# Define the model\n",
    "model = lambda: torch.nn.Sequential(\n",
    "    torch.nn.Linear(M, n_hidden_units),  # M features to n_hidden_units\n",
    "    torch.nn.Tanh(),  # 1st transfer function,\n",
    "    torch.nn.Linear(n_hidden_units, 1),  # n_hidden_units to 1 output neuron\n",
    "    # no final tranfer function, i.e. \"linear output\"\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss()  # notice how this is now a mean-squared-error loss\n",
    "\n",
    "print(\"Training model of type:\\n\\n{}\\n\".format(str(model())))\n",
    "errors = []  # make a list for storing generalizaition error in each loop\n",
    "\n",
    "\n",
    "\n",
    "k = 0\n",
    "for train_index, test_index in CV.split(X, y):\n",
    "    # extract training and test set for current CV fold\n",
    "    X_train = X[train_index]\n",
    "    y_train = y[train_index]\n",
    "    X_test = X[test_index]\n",
    "    y_test = y[test_index]\n",
    "    internal_cross_validation = 10\n",
    "\n",
    "    (\n",
    "        opt_val_err,\n",
    "        opt_lambda,\n",
    "        mean_w_vs_lambda,\n",
    "        train_err_vs_lambda,\n",
    "        test_err_vs_lambda,\n",
    "    ) = rlr_validate(X_train, y_train, lambdas, internal_cross_validation)\n",
    "\n",
    "    # Standardize outer fold based on training set, and save the mean and standard\n",
    "    # deviations since they're part of the model (they would be needed for\n",
    "    # making new predictions) - for brevity we won't always store these in the scripts\n",
    "    mu[k, :] = np.mean(X_train[:, 1:], 0)\n",
    "    sigma[k, :] = np.std(X_train[:, 1:], 0)\n",
    "\n",
    "    X_train[:, 1:] = (X_train[:, 1:] - mu[k, :]) / sigma[k, :]\n",
    "    X_test[:, 1:] = (X_test[:, 1:] - mu[k, :]) / sigma[k, :]\n",
    "\n",
    "    Xty = X_train.T @ y_train\n",
    "    XtX = X_train.T @ X_train\n",
    "\n",
    "    # Compute mean squared error without using the input data at all\n",
    "    Error_train_nofeatures[k] = (\n",
    "        np.square(y_train - y_train.mean()).sum(axis=0) / y_train.shape[0]\n",
    "    )\n",
    "    Error_test_nofeatures[k] = (\n",
    "        np.square(y_test - y_test.mean()).sum(axis=0) / y_test.shape[0]\n",
    "    )\n",
    "\n",
    "    # Estimate weights for the optimal value of lambda, on entire training set\n",
    "    lambdaI = opt_lambda * np.eye(M)\n",
    "    lambdaI[0, 0] = 0  # Do no regularize the bias term\n",
    "    w_rlr[:, k] = np.linalg.solve(XtX + lambdaI, Xty).squeeze()\n",
    "    # Compute mean squared error with regularization with optimal lambda\n",
    "    Error_train_rlr[k] = (\n",
    "        np.square(y_train - X_train @ w_rlr[:, k]).sum(axis=0) / y_train.shape[0]\n",
    "    )\n",
    "    Error_test_rlr[k] = (\n",
    "        np.square(y_test - X_test @ w_rlr[:, k]).sum(axis=0) / y_test.shape[0]\n",
    "    )\n",
    "\n",
    "    # Estimate weights for unregularized linear regression, on entire training set\n",
    "    w_noreg[:, k] = np.linalg.solve(XtX, Xty).squeeze()\n",
    "    # Compute mean squared error without regularization\n",
    "    Error_train[k] = (\n",
    "        np.square(y_train - X_train @ w_noreg[:, k]).sum(axis=0) / y_train.shape[0]\n",
    "    )\n",
    "    Error_test[k] = (\n",
    "        np.square(y_test - X_test @ w_noreg[:, k]).sum(axis=0) / y_test.shape[0]\n",
    "    )\n",
    "    # OR ALTERNATIVELY: you can use sklearn.linear_model module for linear regression:\n",
    "    # m = lm.LinearRegression().fit(X_train, y_train)\n",
    "    # Error_train[k] = np.square(y_train-m.predict(X_train)).sum()/y_train.shape[0]\n",
    "    # Error_test[k] = np.square(y_test-m.predict(X_test)).sum()/y_test.shape[0]\n",
    "\n",
    "    \n",
    "    \n",
    "    # Now we want to make a ANN to model the data\n",
    "    \n",
    "    print(\"\\nCrossvalidation fold: {0}/{1}\".format(k + 1, K))\n",
    "    # Extract training and test set for current CV fold, convert to tensors\n",
    "    X_train = torch.Tensor(X[train_index, :])\n",
    "    y_train = torch.Tensor(y[train_index])\n",
    "    X_test = torch.Tensor(X[test_index, :])\n",
    "    y_test = torch.Tensor(y[test_index])\n",
    "\n",
    "    # Train the net on training data\n",
    "    net, final_loss, learning_curve = train_neural_net(\n",
    "        model,\n",
    "        loss_fn,\n",
    "        X=X_train,\n",
    "        y=y_train,\n",
    "        n_replicates=n_replicates,\n",
    "        max_iter=max_iter,\n",
    "    )\n",
    "\n",
    "    print(\"\\n\\tBest loss: {}\\n\".format(final_loss))\n",
    "\n",
    "    # Determine estimated class labels for test set\n",
    "    y_test_est = net(X_test)\n",
    "\n",
    "    # Determine errors and errors\n",
    "    se = (y_test_est.float() - y_test.float()) ** 2  # squared error\n",
    "    mse = (sum(se).type(torch.float) / len(y_test)).data.numpy()  # mean\n",
    "    errors.append(mse)  # store error rate for current CV fold\n",
    "\n",
    "    # Display the learning curve for the best net in the current fold\n",
    "    (h,) = summaries_axes[0].plot(learning_curve, color=color_list[k])\n",
    "    h.set_label(\"CV fold {0}\".format(k + 1))\n",
    "    summaries_axes[0].set_xlabel(\"Iterations\")\n",
    "    summaries_axes[0].set_xlim((0, max_iter))\n",
    "    summaries_axes[0].set_ylabel(\"Loss\")\n",
    "    summaries_axes[0].set_title(\"Learning curves\")\n",
    "    \n",
    "    \n",
    "    k += 1\n",
    "\n",
    "# Display the MSE across folds\n",
    "errors = [np.mean(error) for error in errors]\n",
    "summaries_axes[1].bar(\n",
    "    np.arange(1, K + 1), np.squeeze(np.asarray(errors)), color=color_list\n",
    ")\n",
    "summaries_axes[1].set_xlabel(\"Fold\")\n",
    "summaries_axes[1].set_xticks(np.arange(1, K + 1))\n",
    "summaries_axes[1].set_ylabel(\"MSE\")\n",
    "summaries_axes[1].set_title(\"Test mean-squared-error\")\n",
    "\n",
    "print(\"Diagram of best neural net in last fold:\")\n",
    "weights = [net[i].weight.data.numpy().T for i in [0, 2]]\n",
    "biases = [net[i].bias.data.numpy() for i in [0, 2]]\n",
    "tf = [str(net[i]) for i in [1, 2]]\n",
    "draw_neural_net(weights, biases, tf, attribute_names=attributeNames)\n",
    "\n",
    "# Print the average classification error rate\n",
    "print(\n",
    "    \"\\nEstimated generalization error, RMSE: {0}\".format(\n",
    "        round(np.sqrt(np.mean(errors)), 4)\n",
    "    )\n",
    ")\n",
    "\n",
    "# When dealing with regression outputs, a simple way of looking at the quality\n",
    "# of predictions visually is by plotting the estimated value as a function of\n",
    "# the true/known value - these values should all be along a straight line \"y=x\",\n",
    "# and if the points are above the line, the model overestimates, whereas if the\n",
    "# points are below the y=x line, then the model underestimates the value\n",
    "plt.figure(figsize=(10, 10))\n",
    "y_est = y_test_est.data.numpy()\n",
    "y_true = y_test.data.numpy()\n",
    "axis_range = [np.min([y_est, y_true]) - 1, np.max([y_est, y_true]) + 1]\n",
    "plt.plot(axis_range, axis_range, \"k--\")\n",
    "plt.plot(y_true, y_est, \"ob\", alpha=0.25)\n",
    "plt.legend([\"Perfect estimation\", \"Model estimations\"])\n",
    "plt.title(\"Alcohol content: estimated versus true value (for last CV-fold)\")\n",
    "plt.ylim(axis_range)\n",
    "plt.xlim(axis_range)\n",
    "plt.xlabel(\"True value\")\n",
    "plt.ylabel(\"Estimated value\")\n",
    "plt.grid()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "show()\n",
    "\n",
    "# Display the results for the best cross-validation fold\n",
    "# finding the index for the lowest test error\n",
    "lowest_error_index = np.argmin(Error_test_rlr)\n",
    "k = lowest_error_index\n",
    "print(\"the lowest error index is: \", k)\n",
    "print(\"all the errors are: \", Error_test_rlr)\n",
    "figure(k, figsize=(12, 8))\n",
    "subplot(1, 2, 1)\n",
    "semilogx(lambdas, mean_w_vs_lambda.T[:, 1:], \".-\")  # Don't plot the bias term\n",
    "xlabel(\"Regularization factor\")\n",
    "ylabel(\"Mean Coefficient Values\")\n",
    "grid()\n",
    "# You can choose to display the legend, but it's omitted for a cleaner\n",
    "# plot, since there are many attributes\n",
    "# legend(attributeNames[1:], loc='best')\n",
    "\n",
    "subplot(1, 2, 2)\n",
    "title(\"Optimal lambda: 1e{0}\".format(np.log10(opt_lambda)))\n",
    "loglog(\n",
    "    lambdas, train_err_vs_lambda.T, \"b.-\", lambdas, test_err_vs_lambda.T, \"r.-\"\n",
    ")\n",
    "xlabel(\"Regularization factor\")\n",
    "ylabel(\"Squared error (crossvalidation)\")\n",
    "legend([\"Train error\", \"Validation error\"])\n",
    "grid()\n",
    "\n",
    "# Display results\n",
    "print(\"Linear regression without feature selection:\")\n",
    "print(\"- Training error: {0}\".format(Error_train.mean()))\n",
    "print(\"- Test error:     {0}\".format(Error_test.mean()))\n",
    "print(\n",
    "    \"- R^2 train:     {0}\".format(\n",
    "        (Error_train_nofeatures.sum() - Error_train.sum())\n",
    "        / Error_train_nofeatures.sum()\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"- R^2 test:     {0}\\n\".format(\n",
    "        (Error_test_nofeatures.sum() - Error_test.sum()) / Error_test_nofeatures.sum()\n",
    "    )\n",
    ")\n",
    "print(\"Regularized linear regression:\")\n",
    "print(\"- Training error: {0}\".format(Error_train_rlr.mean()))\n",
    "print(\"- Test error:     {0}\".format(Error_test_rlr.mean()))\n",
    "print(\n",
    "    \"- R^2 train:     {0}\".format(\n",
    "        (Error_train_nofeatures.sum() - Error_train_rlr.sum())\n",
    "        / Error_train_nofeatures.sum()\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"- R^2 test:     {0}\\n\".format(\n",
    "        (Error_test_nofeatures.sum() - Error_test_rlr.sum())\n",
    "        / Error_test_nofeatures.sum()\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Weights in best fold:\")\n",
    "# print(M)\n",
    "# print(attributeNames)\n",
    "for m in range(M):\n",
    "    print(\"{:>15} {:>15}\".format(attributeNames[m], np.round(w_rlr[m, k], 2)))\n",
    "\n",
    "print(\"Ran Exercise 8.1.1\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "SVD did not converge",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_pca_preprocessing:\n\u001b[1;32m      5\u001b[0m     Y \u001b[38;5;241m=\u001b[39m stats\u001b[38;5;241m.\u001b[39mzscore(X, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m     U, S, V \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msvd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_matrices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     V \u001b[38;5;241m=\u001b[39m V\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Components to be included as features\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/ML/MLenv/lib/python3.12/site-packages/numpy/linalg/_linalg.py:1839\u001b[0m, in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv, hermitian)\u001b[0m\n\u001b[1;32m   1835\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD->DdD\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md->ddd\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1836\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m errstate(call\u001b[38;5;241m=\u001b[39m_raise_linalgerror_svd_nonconvergence,\n\u001b[1;32m   1837\u001b[0m               invalid\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcall\u001b[39m\u001b[38;5;124m'\u001b[39m, over\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m, divide\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1838\u001b[0m               under\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m-> 1839\u001b[0m     u, s, vh \u001b[38;5;241m=\u001b[39m \u001b[43mgufunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1840\u001b[0m u \u001b[38;5;241m=\u001b[39m u\u001b[38;5;241m.\u001b[39mastype(result_t, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1841\u001b[0m s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mastype(_realType(result_t), copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/ML/MLenv/lib/python3.12/site-packages/numpy/linalg/_linalg.py:113\u001b[0m, in \u001b[0;36m_raise_linalgerror_svd_nonconvergence\u001b[0;34m(err, flag)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_raise_linalgerror_svd_nonconvergence\u001b[39m(err, flag):\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSVD did not converge\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mLinAlgError\u001b[0m: SVD did not converge"
     ]
    }
   ],
   "source": [
    "\n",
    "## Normalize and compute PCA (change to True to experiment with PCA preprocessing)\n",
    "do_pca_preprocessing = True\n",
    "if do_pca_preprocessing:\n",
    "    Y = stats.zscore(X, 0)\n",
    "    U, S, V = np.linalg.svd(Y, full_matrices=False)\n",
    "    V = V.T\n",
    "    # Components to be included as features\n",
    "    k_pca = 3\n",
    "    X = X @ V[:, :k_pca]\n",
    "    N, M = X.shape\n",
    "\n",
    "\n",
    "# Parameters for neural network classifier\n",
    "n_hidden_units = 2  # number of hidden units\n",
    "n_replicates = 1  # number of networks trained in each k-fold\n",
    "max_iter = 10000\n",
    "\n",
    "# K-fold crossvalidation\n",
    "K = 3  # only three folds to speed up this example\n",
    "CV = model_selection.KFold(K, shuffle=True)\n",
    "\n",
    "# Setup figure for display of learning curves and error rates in fold\n",
    "summaries, summaries_axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "# Make a list for storing assigned color of learning curve for up to K=10\n",
    "color_list = [\n",
    "    \"tab:orange\",\n",
    "    \"tab:green\",\n",
    "    \"tab:purple\",\n",
    "    \"tab:brown\",\n",
    "    \"tab:pink\",\n",
    "    \"tab:gray\",\n",
    "    \"tab:olive\",\n",
    "    \"tab:cyan\",\n",
    "    \"tab:red\",\n",
    "    \"tab:blue\",\n",
    "]\n",
    "# Define the model\n",
    "model = lambda: torch.nn.Sequential(\n",
    "    torch.nn.Linear(M, n_hidden_units),  # M features to n_hidden_units\n",
    "    torch.nn.Tanh(),  # 1st transfer function,\n",
    "    torch.nn.Linear(n_hidden_units, 1),  # n_hidden_units to 1 output neuron\n",
    "    # no final tranfer function, i.e. \"linear output\"\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss()  # notice how this is now a mean-squared-error loss\n",
    "\n",
    "print(\"Training model of type:\\n\\n{}\\n\".format(str(model())))\n",
    "errors = []  # make a list for storing generalizaition error in each loop\n",
    "for k, (train_index, test_index) in enumerate(CV.split(X, y)):\n",
    "    print(\"\\nCrossvalidation fold: {0}/{1}\".format(k + 1, K))\n",
    "    \n",
    "    # Extract training and test set for current CV fold, convert to tensors\n",
    "    X_train = torch.Tensor(X[train_index, :])\n",
    "    y_train = torch.Tensor(y[train_index])\n",
    "    X_test = torch.Tensor(X[test_index, :])\n",
    "    y_test = torch.Tensor(y[test_index])\n",
    "\n",
    "    # Train the net on training data\n",
    "    net, final_loss, learning_curve = train_neural_net(\n",
    "        model,\n",
    "        loss_fn,\n",
    "        X=X_train,\n",
    "        y=y_train,\n",
    "        n_replicates=n_replicates,\n",
    "        max_iter=max_iter,\n",
    "    )\n",
    "\n",
    "    print(\"\\n\\tBest loss: {}\\n\".format(final_loss))\n",
    "\n",
    "    # Determine estimated class labels for test set\n",
    "    y_test_est = net(X_test)\n",
    "\n",
    "    # Determine errors and errors\n",
    "    se = (y_test_est.float() - y_test.float()) ** 2  # squared error\n",
    "    mse = (sum(se).type(torch.float) / len(y_test)).data.numpy()  # mean\n",
    "    errors.append(mse)  # store error rate for current CV fold\n",
    "\n",
    "    # Display the learning curve for the best net in the current fold\n",
    "    (h,) = summaries_axes[0].plot(learning_curve, color=color_list[k])\n",
    "    h.set_label(\"CV fold {0}\".format(k + 1))\n",
    "    summaries_axes[0].set_xlabel(\"Iterations\")\n",
    "    summaries_axes[0].set_xlim((0, max_iter))\n",
    "    summaries_axes[0].set_ylabel(\"Loss\")\n",
    "    summaries_axes[0].set_title(\"Learning curves\")\n",
    "\n",
    "# Display the MSE across folds\n",
    "summaries_axes[1].bar(\n",
    "    np.arange(1, K + 1), np.squeeze(np.asarray(errors)), color=color_list\n",
    ")\n",
    "summaries_axes[1].set_xlabel(\"Fold\")\n",
    "summaries_axes[1].set_xticks(np.arange(1, K + 1))\n",
    "summaries_axes[1].set_ylabel(\"MSE\")\n",
    "summaries_axes[1].set_title(\"Test mean-squared-error\")\n",
    "\n",
    "print(\"Diagram of best neural net in last fold:\")\n",
    "weights = [net[i].weight.data.numpy().T for i in [0, 2]]\n",
    "biases = [net[i].bias.data.numpy() for i in [0, 2]]\n",
    "tf = [str(net[i]) for i in [1, 2]]\n",
    "draw_neural_net(weights, biases, tf, attribute_names=attributeNames)\n",
    "\n",
    "# Print the average classification error rate\n",
    "print(\n",
    "    \"\\nEstimated generalization error, RMSE: {0}\".format(\n",
    "        round(np.sqrt(np.mean(errors)), 4)\n",
    "    )\n",
    ")\n",
    "\n",
    "# When dealing with regression outputs, a simple way of looking at the quality\n",
    "# of predictions visually is by plotting the estimated value as a function of\n",
    "# the true/known value - these values should all be along a straight line \"y=x\",\n",
    "# and if the points are above the line, the model overestimates, whereas if the\n",
    "# points are below the y=x line, then the model underestimates the value\n",
    "plt.figure(figsize=(10, 10))\n",
    "y_est = y_test_est.data.numpy()\n",
    "y_true = y_test.data.numpy()\n",
    "axis_range = [np.min([y_est, y_true]) - 1, np.max([y_est, y_true]) + 1]\n",
    "plt.plot(axis_range, axis_range, \"k--\")\n",
    "plt.plot(y_true, y_est, \"ob\", alpha=0.25)\n",
    "plt.legend([\"Perfect estimation\", \"Model estimations\"])\n",
    "plt.title(\"Alcohol content: estimated versus true value (for last CV-fold)\")\n",
    "plt.ylim(axis_range)\n",
    "plt.xlim(axis_range)\n",
    "plt.xlabel(\"True value\")\n",
    "plt.ylabel(\"Estimated value\")\n",
    "plt.grid()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Ran Exercise 8.2.5\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
