{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg as linalg\n",
    "from dtuimldmtools import similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import svd\n",
    "from pandas.plotting import scatter_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.linear_model as lm\n",
    "from matplotlib.pyplot import figure, legend, plot, show, xlabel, ylabel\n",
    "# exercise 8.1.1\n",
    "from dtuimldmtools import dbplotf, train_neural_net, visualize_decision_boundary\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from collections import Counter\n",
    "# from sklearn.neural_network import MLPRegress\n",
    "import importlib_resources\n",
    "import numpy as np\n",
    "import sklearn.linear_model as lm\n",
    "from matplotlib.pylab import (\n",
    "    figure,\n",
    "    grid,\n",
    "    legend,\n",
    "    loglog,\n",
    "    semilogx,\n",
    "    show,\n",
    "    subplot,\n",
    "    title,\n",
    "    xlabel,\n",
    "    ylabel,\n",
    ")\n",
    "from scipy.io import loadmat\n",
    "from sklearn import model_selection\n",
    "\n",
    "from dtuimldmtools import rlr_validate\n",
    "import numpy as np\n",
    "import scipy.linalg as linalg\n",
    "from dtuimldmtools import similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import svd\n",
    "from pandas.plotting import scatter_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.linear_model as lm\n",
    "from matplotlib.pyplot import figure, legend, plot, show, xlabel, ylabel\n",
    "# exercise 8.1.1\n",
    "from dtuimldmtools import dbplotf, train_neural_net, visualize_decision_boundary\n",
    "import importlib_resources\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib.pyplot import figure, show, title\n",
    "from scipy.io import loadmat\n",
    "import importlib_resources\n",
    "import numpy as np\n",
    "import sklearn.linear_model as lm\n",
    "from matplotlib.pylab import (\n",
    "    figure,\n",
    "    grid,\n",
    "    legend,\n",
    "    loglog,\n",
    "    semilogx,\n",
    "    show,\n",
    "    subplot,\n",
    "    title,\n",
    "    xlabel,\n",
    "    ylabel,\n",
    ")\n",
    "from scipy.io import loadmat\n",
    "from sklearn import model_selection\n",
    "\n",
    "from dtuimldmtools import rlr_validate\n",
    "\n",
    "# fetch dataset \n",
    "# wine = fetch_ucirepo(id=109) \n",
    "  \n",
    "# # data (as pandas dataframes) \n",
    "# X = wine.data.features \n",
    "# y = wine.data.targets \n",
    "\n",
    "# totaldata= (wine.data)\n",
    "  \n",
    "# metadata \n",
    "# print(wine.metadata) \n",
    "  \n",
    "# variable information \n",
    "# print(wine.variables) \n",
    "\n",
    "# OFFLINE LOADING OF DATA\n",
    "X = np.loadtxt('../wine/wine.data', delimiter=',')\n",
    "\n",
    "#Extract classes from X\n",
    "y = X[:,0]\n",
    "X = np.delete(X,0,axis=1)\n",
    "# y = np.loadtxt('../wine/wine.names', delimiter=',')\n",
    "\n",
    "Xorig = X\n",
    "# Standardizing the data\n",
    "\n",
    "X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "attributeNames = [\n",
    "    \"Alcohol\",\n",
    "    \"Malic acid\",\n",
    "    \"Ash\",\n",
    "    \"Alcalinity of ash\",\n",
    "    \"Magnesium\",\n",
    "    \"Total phenols\",\n",
    "    \"Flavanoids\",\n",
    "    \"Nonflavanoid phenols\",\n",
    "    \"Proanthocyanins\",\n",
    "    \"Color intensity\",\n",
    "    \"Hue\",\n",
    "    \"OD280/OD315 of diluted wines\",\n",
    "    \"Proline\"\n",
    "]\n",
    "\n",
    "N, M = X.shape\n",
    "\n",
    "# Add offset attribute\n",
    "attributeNames = [\"Offset\"] + attributeNames\n",
    "# M = M + 1\n",
    "\n",
    "\n",
    "# Convert class labels to 0, 1, 2\n",
    "y = y - 1\n",
    "\n",
    "classNames = [\"0\", \"1\", \"2\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer Fold:  1\n",
      "Number of hidden units:  1\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.8586534\t6.434488e-05\n",
      "\t\t2000\t0.8163155\t3.1469197e-05\n",
      "\t\t3000\t0.8008\t1.0866862e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.8008\t1.0866862e-05\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lauge/Desktop/ML/MLenv/lib/python3.12/site-packages/dtuimldmtools/models/nn_trainer.py:141: RuntimeWarning: overflow encountered in cast\n",
      "  if loss_value < best_final_loss:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t1000\t0.8350412\t0.000108342014\n",
      "\t\t2000\t0.79656476\t2.3495162e-05\n",
      "\t\t3000\t0.7813111\t1.441822e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.7813111\t1.441822e-05\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.8634782\t4.0932253e-05\n",
      "\t\t2000\t0.8342418\t3.0650117e-05\n",
      "\t\t3000\t0.72802216\t0.0001112517\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.72802216\t0.0001112517\n",
      "Number of hidden units:  2\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.85447514\t0.00010378594\n",
      "\t\t2000\t0.5939797\t0.00010545457\n",
      "\t\t3000\t0.56598127\t2.1693812e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.56598127\t2.1693812e-05\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.6313383\t0.00030400214\n",
      "\t\t2000\t0.57209134\t3.542242e-05\n",
      "\t\t3000\t0.56040215\t1.1593158e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.56040215\t1.1593158e-05\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.6661591\t0.00020422992\n",
      "\t\t2000\t0.5756679\t4.1517807e-05\n",
      "\t\t3000\t0.5619349\t1.3470756e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.5619349\t1.3470756e-05\n",
      "Outer Fold:  1\n",
      "Number of hidden units:  1\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.8798399\t4.5116052e-05\n",
      "\t\t2000\t0.7921234\t9.8488184e-05\n",
      "\t\t3000\t0.7539157\t2.7274988e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.7539157\t2.7274988e-05\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lauge/Desktop/ML/MLenv/lib/python3.12/site-packages/dtuimldmtools/models/nn_trainer.py:141: RuntimeWarning: overflow encountered in cast\n",
      "  if loss_value < best_final_loss:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t1000\t0.8158677\t0.0001235237\n",
      "\t\t2000\t0.77119315\t2.465454e-05\n",
      "\t\t3000\t0.75931776\t1.0440076e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.75931776\t1.0440076e-05\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.8099911\t0.00012324267\n",
      "\t\t2000\t0.7571356\t5.52611e-05\n",
      "\t\t3000\t0.68988216\t6.203013e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.68988216\t6.203013e-05\n",
      "Number of hidden units:  2\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.79998\t7.4576616e-05\n",
      "\t\t2000\t0.71927106\t0.001118131\n",
      "\t\t3000\t0.5601281\t1.7238532e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.5601281\t1.7238532e-05\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.6372131\t0.00020630573\n",
      "\t\t2000\t0.5699174\t3.493007e-05\n",
      "\t\t3000\t0.55906796\t9.381971e-06\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.55906796\t9.381971e-06\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.62872934\t0.00015042775\n",
      "\t\t2000\t0.58558553\t2.9313625e-05\n",
      "\t\t3000\t0.5758633\t9.108344e-06\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.5758633\t9.108344e-06\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.6015245\t0.00012988922\n",
      "\t\t2000\t0.5684579\t1.8139279e-05\n",
      "\t\t3000\t0.5628223\t5.2951286e-06\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.5628223\t5.2951286e-06\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.62291014\t0.00016522482\n",
      "\t\t2000\t0.57091916\t3.612151e-05\n",
      "\t\t3000\t0.5594003\t1.1081178e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.5594003\t1.1081178e-05\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.6166186\t0.0002473979\n",
      "\t\t2000\t0.5680117\t2.9695891e-05\n",
      "\t\t3000\t0.5583858\t9.606918e-06\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.5583858\t9.606918e-06\n",
      "Number of miss-classifications for ANN:\n",
      "\t 1 out of 60\n",
      "Best hidden unit:  2\n",
      "final ANN error for k=1 is 0.016666666666666666\n",
      "the best lambda is:  1.0\n",
      "final logreg error for k=1 is 0.016666666666666666\n",
      "   Top class:  2.0   Test error:  0.667\n",
      "Outer Fold:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lauge/Desktop/ML/MLenv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hidden units:  1\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.8572061\t0.00016601873\n",
      "\t\t2000\t0.74006724\t0.00010557614\n",
      "\t\t3000\t0.68843675\t5.350339e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.68843675\t5.350339e-05\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lauge/Desktop/ML/MLenv/lib/python3.12/site-packages/dtuimldmtools/models/nn_trainer.py:141: RuntimeWarning: overflow encountered in cast\n",
      "  if loss_value < best_final_loss:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t1000\t0.90824336\t0.00019834893\n",
      "\t\t2000\t0.85813135\t2.1462261e-05\n",
      "\t\t3000\t0.8465164\t7.886047e-06\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.8465164\t7.886047e-06\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.8396454\t7.701592e-05\n",
      "\t\t2000\t0.73979384\t0.00010263471\n",
      "\t\t3000\t0.69013554\t4.4131375e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.69013554\t4.4131375e-05\n",
      "Number of hidden units:  2\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7142111\t0.0006611955\n",
      "\t\t2000\t0.5918737\t3.776295e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2499\t0.579924\t4.1112023e-07\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.6081864\t0.00013424733\n",
      "\t\t2000\t0.5694747\t2.9305624e-05\n",
      "\t\t3000\t0.5595541\t1.0225977e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.5595541\t1.0225977e-05\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.59451336\t0.00012069591\n",
      "\t\t2000\t0.56342435\t2.0945976e-05\n",
      "\t\t3000\t0.55663013\t6.853152e-06\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.55663013\t6.853152e-06\n",
      "Outer Fold:  2\n",
      "Number of hidden units:  1\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.7904016\t8.679005e-05\n",
      "\t\t2000\t0.75367826\t2.8469789e-05\n",
      "\t\t3000\t0.731881\t3.241221e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.731881\t3.241221e-05\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lauge/Desktop/ML/MLenv/lib/python3.12/site-packages/dtuimldmtools/models/nn_trainer.py:141: RuntimeWarning: overflow encountered in cast\n",
      "  if loss_value < best_final_loss:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t1000\t0.77797294\t0.000148918\n",
      "\t\t2000\t0.72302437\t3.190247e-05\n",
      "\t\t3000\t0.70747083\t4.19549e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.70747083\t4.19549e-05\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.76003927\t0.00015548884\n",
      "\t\t2000\t0.70176065\t2.3186953e-05\n",
      "\t\t3000\t0.68288857\t3.9624967e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.68288857\t3.9624967e-05\n",
      "Number of hidden units:  2\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.74971825\t0.0003458762\n",
      "\t\t2000\t0.5888011\t7.2779454e-05\n",
      "\t\t3000\t0.5664482\t2.0413278e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.5664482\t2.0413278e-05\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.6287463\t0.0002509653\n",
      "\t\t2000\t0.57249564\t3.4252236e-05\n",
      "\t\t3000\t0.5609734\t1.1687603e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.5609734\t1.1687603e-05\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.6569484\t0.00012265133\n",
      "\t\t2000\t0.5865482\t7.722474e-05\n",
      "\t\t3000\t0.56385905\t1.9555673e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.56385905\t1.9555673e-05\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.6217051\t0.00016209472\n",
      "\t\t2000\t0.57700616\t3.0162644e-05\n",
      "\t\t3000\t0.56647044\t9.680248e-06\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.56647044\t9.680248e-06\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.6135141\t0.00015046712\n",
      "\t\t2000\t0.5705475\t3.2279942e-05\n",
      "\t\t3000\t0.55960304\t1.1077163e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.55960304\t1.1077163e-05\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.6233556\t0.00015612146\n",
      "\t\t2000\t0.57479364\t3.815921e-05\n",
      "\t\t3000\t0.56176656\t1.3262594e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.56176656\t1.3262594e-05\n",
      "Number of miss-classifications for ANN:\n",
      "\t 1 out of 59\n",
      "Best hidden unit:  2\n",
      "final ANN error for k=2 is 0.01694915254237288\n",
      "the best lambda is:  1.0\n",
      "final logreg error for k=2 is 0.0\n",
      "   Top class:  2.0   Test error:  0.644\n",
      "Outer Fold:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lauge/Desktop/ML/MLenv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hidden units:  1\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.816757\t6.5237364e-05\n",
      "\t\t2000\t0.77032375\t4.2168194e-05\n",
      "\t\t3000\t0.73458755\t2.1582851e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.73458755\t2.1582851e-05\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lauge/Desktop/ML/MLenv/lib/python3.12/site-packages/dtuimldmtools/models/nn_trainer.py:141: RuntimeWarning: overflow encountered in cast\n",
      "  if loss_value < best_final_loss:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t1000\t0.8662357\t7.9467856e-05\n",
      "\t\t2000\t0.82907283\t2.7462424e-05\n",
      "\t\t3000\t0.80730915\t1.9934041e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.80730915\t1.9934041e-05\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.8238228\t4.9268805e-05\n",
      "\t\t2000\t0.7905402\t0.00010614821\n",
      "\t\t3000\t0.7486977\t2.1812964e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.7486977\t2.1812964e-05\n",
      "Number of hidden units:  2\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.60199344\t0.00015908715\n",
      "\t\t2000\t0.5662376\t2.4210218e-05\n",
      "\t\t3000\t0.5581192\t8.4367775e-06\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.5581192\t8.4367775e-06\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.6408215\t0.00022504048\n",
      "\t\t2000\t0.57599026\t4.283973e-05\n",
      "\t\t3000\t0.5618867\t1.36840645e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.5618867\t1.36840645e-05\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.6006616\t0.00014118671\n",
      "\t\t2000\t0.56562096\t2.3815117e-05\n",
      "\t\t3000\t0.55773455\t8.015126e-06\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.55773455\t8.015126e-06\n",
      "Outer Fold:  3\n",
      "Number of hidden units:  1\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.9199864\t0.0001146627\n",
      "\t\t2000\t0.82217133\t6.850462e-05\n",
      "\t\t3000\t0.78601044\t2.8663626e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.78601044\t2.8663626e-05\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lauge/Desktop/ML/MLenv/lib/python3.12/site-packages/dtuimldmtools/models/nn_trainer.py:141: RuntimeWarning: overflow encountered in cast\n",
      "  if loss_value < best_final_loss:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t1000\t0.823382\t7.433903e-05\n",
      "\t\t2000\t0.77583545\t6.7064946e-05\n",
      "\t\t3000\t0.7289394\t5.0530685e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.7289394\t5.0530685e-05\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.78150874\t5.9257258e-05\n",
      "\t\t2000\t0.7316156\t7.144399e-05\n",
      "\t\t3000\t0.69330245\t3.7826278e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.69330245\t3.7826278e-05\n",
      "Number of hidden units:  2\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.62324816\t0.00018970482\n",
      "\t\t2000\t0.5698957\t3.3258097e-05\n",
      "\t\t3000\t0.5592835\t1.0230925e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.5592835\t1.0230925e-05\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.63610375\t0.00020432385\n",
      "\t\t2000\t0.5775026\t4.2211534e-05\n",
      "\t\t3000\t0.5630955\t1.4501479e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.5630955\t1.4501479e-05\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.67448425\t0.00035388774\n",
      "\t\t2000\t0.5827593\t5.8193953e-05\n",
      "\t\t3000\t0.56403524\t1.7330463e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.56403524\t1.7330463e-05\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.630784\t0.00018649432\n",
      "\t\t2000\t0.57439953\t3.9638042e-05\n",
      "\t\t3000\t0.5613338\t1.2848094e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.5613338\t1.2848094e-05\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.6658384\t0.00026839296\n",
      "\t\t2000\t0.5804499\t5.2675645e-05\n",
      "\t\t3000\t0.56351674\t1.5865642e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.56351674\t1.5865642e-05\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.6366606\t0.00017709911\n",
      "\t\t2000\t0.57849765\t4.461152e-05\n",
      "\t\t3000\t0.5633693\t1.5446607e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.5633693\t1.5446607e-05\n",
      "Number of miss-classifications for ANN:\n",
      "\t 4 out of 59\n",
      "Best hidden unit:  2\n",
      "final ANN error for k=3 is 0.06779661016949153\n",
      "the best lambda is:  1.0\n",
      "final logreg error for k=3 is 0.0\n",
      "   Top class:  1.0   Test error:  0.695\n",
      "final_error_for_ANN:  [np.float64(0.016666666666666666), np.float64(0.01694915254237288), np.float64(0.06779661016949153)]\n",
      "best_hidden_units:  [2, 2, 2]\n",
      "final_error_for_logreg:  [np.float64(0.016666666666666666), np.float64(0.0), np.float64(0.0)]\n",
      "best_lambdas:  [np.float64(1.0), np.float64(1.0), np.float64(1.0)]\n",
      "final_error_for_baseline:  [[0.66666667]\n",
      " [0.6440678 ]\n",
      " [0.69491525]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lauge/Desktop/ML/MLenv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# %% Model fitting and prediction using logistic regression\n",
    "\n",
    "## Crossvalidation\n",
    "# Create OUTER crossvalidation partition for evaluation\n",
    "K = 10\n",
    "CV = model_selection.KFold(K, shuffle=True, random_state=12)\n",
    "# CV = model_selection.KFold(K, shuffle=False)\n",
    "\n",
    "# Values of lambda\n",
    "lambdas = np.logspace(0, 3, 100)\n",
    "# Initialize variables\n",
    "# T = len(lambdas)\n",
    "Error_train = np.empty((K, 1))\n",
    "Error_test = np.empty((K, 1))\n",
    "Error_train_rlr = np.empty((K, 1))\n",
    "Error_test_rlr = np.empty((K, 1))\n",
    "Error_train_nofeatures = np.empty((K, 1))\n",
    "Error_test_nofeatures = np.empty((K, 1))\n",
    "w_rlr = np.empty((M, K))\n",
    "mu = np.empty((K, M - 1))\n",
    "sigma = np.empty((K, M - 1))\n",
    "w_noreg = np.empty((M, K))\n",
    "final_error_for_ANN = []\n",
    "final_error_for_logreg = []\n",
    "best_hidden_units = []\n",
    "best_lambdas = []\n",
    "\n",
    "\n",
    "k = 0\n",
    "for train_index, test_index in CV.split(X, y):\n",
    "    k += 1\n",
    "    Error_train_avg = []\n",
    "    Error_test_avg = []\n",
    "    # extract training and test set for current CV fold\n",
    "    X_train = X[train_index]\n",
    "    y_train = y[train_index]\n",
    "    X_test = X[test_index]\n",
    "    y_test = y[test_index]\n",
    "    internal_cross_validation = 10\n",
    "    \n",
    "    internal_cv = model_selection.KFold(internal_cross_validation, shuffle=True, random_state=42)\n",
    "\n",
    "    # Inner cross-validation to find the best regularization parameter (lambda)\n",
    "    best_lambda = 0.0\n",
    "    min_error = float('inf')\n",
    "    for train_inner_idx, val_inner_idx in internal_cv.split(X_train, y_train):\n",
    "        print(\"Outer Fold: \", k)\n",
    "        #FOR LOGISTIC REGRESSION\n",
    "        for l in lambdas:\n",
    "            Error_train_rlr_fold = []\n",
    "            Error_test_rlr_fold = []\n",
    "            error_inner = []\n",
    "        # For each lambda we do the training with internal cross validation\n",
    "            X_train_inner, y_train_inner = X_train[train_inner_idx], y_train[train_inner_idx]\n",
    "            \n",
    "            X_val_inner, y_val_inner = X_train[val_inner_idx], y_train[val_inner_idx]\n",
    "\n",
    "            logreg = lm.LogisticRegression(C=1/l, solver=\"lbfgs\", tol=1e-4, random_state=1)\n",
    "            logreg.fit(X_train_inner, y_train_inner)\n",
    "            \n",
    "            error_val = np.mean(logreg.predict(X_val_inner) != y_val_inner)\n",
    "\n",
    "        # If the error is less than the minimum error, we update the best lambda\n",
    "            # if error_val < min_error:\n",
    "            #     min_error = error_val\n",
    "            #     best_lambda = l\n",
    "                \n",
    "            error_train_new = np.mean(logreg.predict(X_train_inner) != y_train_inner)\n",
    "            Error_train_rlr_fold.append(error_train_new)\n",
    "            Error_test_rlr_fold.append(error_val)\n",
    "            \n",
    "        #_____________________________________________________\n",
    "        # FOR ANN\n",
    "        # Define the model structure\n",
    "        max_hidden_units = 10\n",
    "        error_log = []\n",
    "        for n_hidden_units in range(5, max_hidden_units):\n",
    "            print(\"Number of hidden units: \", n_hidden_units)\n",
    "            # number of hidden units in the signle hidden layer\n",
    "            C = 3\n",
    "            model = lambda: torch.nn.Sequential(\n",
    "                torch.nn.Linear(M, n_hidden_units),  # M features to H hiden units\n",
    "                torch.nn.ReLU(),  # 1st transfer function\n",
    "                # Output layer:\n",
    "                # H hidden units to C classes\n",
    "                # the nodes and their activation before the transfer\n",
    "                # function is often referred to as logits/logit output\n",
    "                torch.nn.Linear(n_hidden_units, C),  # C logits\n",
    "                # To obtain normalised \"probabilities\" of each class\n",
    "                # we use the softmax-funtion along the \"class\" dimension\n",
    "                # (i.e. not the dimension describing observations)\n",
    "                torch.nn.Softmax(dim=1),  # final tranfer function, normalisation of logit output\n",
    "            )\n",
    "            # Since we're training a multiclass problem, we cannot use binary cross entropy,\n",
    "            # but instead use the general cross entropy loss:\n",
    "            loss_fn = torch.nn.CrossEntropyLoss()\n",
    "            # Train the network:\n",
    "            # C = 3\n",
    "            net, _, _ = train_neural_net(\n",
    "                model,\n",
    "                loss_fn,\n",
    "                X=torch.tensor(X_train_inner, dtype=torch.float),\n",
    "                y=torch.tensor(y_train_inner, dtype=torch.long),\n",
    "                n_replicates=3,\n",
    "                max_iter=3000\n",
    "            )\n",
    "            # Determine probability of each class using trained network\n",
    "            softmax_logits = net(torch.tensor(X_val_inner, dtype=torch.float))\n",
    "            # Get the estimated class as the class with highest probability (argmax on softmax_logits)\n",
    "            y_test_est = (torch.max(softmax_logits, dim=1)[1]).data.numpy()\n",
    "            # Determine errors\n",
    "            e = y_test_est != y_val_inner\n",
    "            # print(\n",
    "            #     \"Number of miss-classifications for ANN:\\n\\t {0} out of {1}\".format(sum(e), len(e))\n",
    "            # )\n",
    "            error_log.append(np.mean(e))\n",
    "\n",
    "            predict = lambda x: (\n",
    "                torch.max(net(torch.tensor(x, dtype=torch.float)), dim=1)[1]\n",
    "            ).data.numpy()\n",
    "        #Find the best amount of hidden units\n",
    "        # print(error_log)\n",
    "        # print(range(1, max_hidden_units))\n",
    "        # print(np.argmin(error_log))\n",
    "        best_hidden_unit = range(1, max_hidden_units)[np.argmin(error_log)]\n",
    "    \n",
    "    #For ANN We just train with the best \n",
    "    #_____________________________________________________\n",
    "    # Average errors across folds for each hidden unit\n",
    "    \n",
    "    C = 3\n",
    "    model = lambda: torch.nn.Sequential(\n",
    "        torch.nn.Linear(M, best_hidden_unit),  # M features to H hiden units\n",
    "        torch.nn.ReLU(),  # 1st transfer function\n",
    "        # Output layer:\n",
    "        # H hidden units to C classes\n",
    "        # the nodes and their activation before the transfer\n",
    "        # function is often referred to as logits/logit output\n",
    "        torch.nn.Linear(best_hidden_unit, C),  # C logits\n",
    "        # To obtain normalised \"probabilities\" of each class\n",
    "        # we use the softmax-funtion along the \"class\" dimension\n",
    "        # (i.e. not the dimension describing observations)\n",
    "        torch.nn.Softmax(dim=1),  # final tranfer function, normalisation of logit output\n",
    "    )\n",
    "    # Since we're training a multiclass problem, we cannot use binary cross entropy,\n",
    "    # but instead use the general cross entropy loss:\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    # Train the network:\n",
    "    # C = 3\n",
    "    net, _, _ = train_neural_net(\n",
    "        model,\n",
    "        loss_fn,\n",
    "        X=torch.tensor(X_train, dtype=torch.float),\n",
    "        y=torch.tensor(y_train, dtype=torch.long),\n",
    "        n_replicates=3,\n",
    "        max_iter=3000\n",
    "    )\n",
    "    # Determine probability of each class using trained network\n",
    "    softmax_logits = net(torch.tensor(X_test, dtype=torch.float))\n",
    "    # Get the estimated class as the class with highest probability (argmax on softmax_logits)\n",
    "    y_test_est = (torch.max(softmax_logits, dim=1)[1]).data.numpy()\n",
    "    # Determine errors\n",
    "    print(\n",
    "        \"Number of miss-classifications for ANN:\\n\\t {0} out of {1}\".format(sum(y_test_est != y_test), len(y_test))\n",
    "    )\n",
    "    \n",
    "    final_error_for_ANN.append(np.mean(y_test_est != y_test))\n",
    "    predict = lambda x: (\n",
    "        torch.max(net(torch.tensor(x, dtype=torch.float)), dim=1)[1]\n",
    "    ).data.numpy()\n",
    "    best_hidden_units.append(best_hidden_unit)\n",
    "    print(\"Best hidden unit: \", best_hidden_unit)\n",
    "    print(\"final ANN error for k=\" + str(k) + \" is \" + str(np.mean(y_test_est != y_test)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    #FOR LAMBDA / LOGISTIC REGRESSION\n",
    "    #_____________________________________________________\n",
    "    # Average errors across folds for each lambda\n",
    "    Error_test_avg.append(np.mean(Error_test_rlr_fold))\n",
    "    \n",
    "    best_lambda = lambdas[np.argmin(Error_test_avg)]\n",
    "    \n",
    "    \n",
    "    print(\"the best lambda is: \", best_lambda)\n",
    "    \n",
    "    \n",
    "    # Train with best lambda on outer fold and evaluate amount of errors\n",
    "    logreg_best = lm.LogisticRegression(C=best_lambda, solver=\"lbfgs\",  tol=1e-4, random_state=1)\n",
    "    logreg_best.fit(X_train, y_train)\n",
    "\n",
    "    # Multinomial logistic regression\n",
    "    logreg = lm.LogisticRegression(\n",
    "        C=best_lambda,solver=\"lbfgs\", multi_class=\"multinomial\", tol=1e-4, random_state=1\n",
    "    )\n",
    "    logreg.fit(X_train, y_train)\n",
    "    best_lambdas.append(best_lambda)\n",
    "    final_error_for_logreg.append(np.mean(logreg.predict(X_test) != y_test))\n",
    "    print(\"final logreg error for k=\" + str(k) + \" is \" + str(np.mean(logreg.predict(X_test) != y_test)))\n",
    "\n",
    "    #_____________________________________________________\n",
    "    \n",
    "    #For the baseline\n",
    "    #_____________________________________________________\n",
    "    most_common_class = Counter(y_train).most_common(1)[0][0]\n",
    "\n",
    "    # Predict the most common class for all test and train samples\n",
    "    y_est_train_base = np.full(len(y_train), most_common_class)\n",
    "    y_est_test_base = np.full(len(y_test), most_common_class)\n",
    "\n",
    "    # Evaluate misclassification rate for train and test sets\n",
    "    misclass_rate_train = np.sum(y_est_train_base != y_train) / len(y_train)\n",
    "    misclass_rate_test = np.sum(y_est_test_base != y_test) / len(y_test)\n",
    "\n",
    "\n",
    "    # Store the errors\n",
    "    Error_train_nofeatures[k-1] = misclass_rate_train\n",
    "    Error_test_nofeatures[k-1] = misclass_rate_test\n",
    "\n",
    "    print(\"   Top class: \", most_common_class+1, \"  Test error: \", round(misclass_rate_test,3))\n",
    "    \n",
    "    #_____________________________________________________\n",
    "    \n",
    "    \n",
    "print(\"final_error_for_ANN: \", (final_error_for_ANN))\n",
    "print(\"best_hidden_units: \", (best_hidden_units))\n",
    "print(\"final_error_for_logreg: \", (final_error_for_logreg))\n",
    "print(\"best_lambdas: \", (best_lambdas))\n",
    "print(\"final_error_for_baseline: \", (Error_test_nofeatures))\n",
    "# Create a matrix from the arrays\n",
    "# matrix = np.column_stack((Error_test, Error_test_avg, Error_test_nofeatures, Error_test_rlr, Error_test_rlr_fold, Error_train, Error_train_avg, Error_train_nofeatures, Error_train_rlr, Error_train_rlr_fold))\n",
    "\n",
    "# Output the matrix to the terminal\n",
    "\n",
    "# Save the matrix to a txt file\n",
    "# np.savetxt('output_matrix.txt', matrix, delimiter=',', fmt='%s')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# To display coefficients use print(logreg.coef_). For a 4 class problem with a\n",
    "# feature space, these weights will have shape (4, 2).\n",
    "\n",
    "# Number of miss-classifications\n",
    "# print(\n",
    "#     \"Number of miss-classifications for Multinormal regression:\\n\\t {0} out of {1}\".format(\n",
    "#         np.sum(logreg.predict(X_test) != y_test), len(y_test)\n",
    "#     )\n",
    "# )\n",
    "# print(logreg.predict(X_test))\n",
    "# print(y_test)\n",
    "\n",
    "# print(\"Coefficients\")\n",
    "# print(logreg.coef_)\n",
    "# # Plotting should be done after the outer loop\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.semilogx(lambdas, Error_train_avg, 'b-', label='Training Error')\n",
    "# plt.semilogx(lambdas, Error_test_avg, 'r-', label='Test Error')\n",
    "# plt.xlabel('Lambda')\n",
    "# plt.ylabel('Error')\n",
    "# plt.legend()\n",
    "# plt.title('Training vs Test Error for Different Lambda Values')\n",
    "# plt.grid()\n",
    "# plt.show()\n",
    "# k += 1\n",
    "# print(\"final_error_for_ANN: \", (final_error_for_ANN))\n",
    "# print(\"final_error_for_logreg: \", (final_error_for_logreg))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
