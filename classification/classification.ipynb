{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg as linalg\n",
    "from dtuimldmtools import similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import svd\n",
    "from pandas.plotting import scatter_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.linear_model as lm\n",
    "from matplotlib.pyplot import figure, legend, plot, show, xlabel, ylabel\n",
    "# exercise 8.1.1\n",
    "from dtuimldmtools import dbplotf, train_neural_net, visualize_decision_boundary\n",
    "\n",
    "import importlib_resources\n",
    "import numpy as np\n",
    "import sklearn.linear_model as lm\n",
    "from matplotlib.pylab import (\n",
    "    figure,\n",
    "    grid,\n",
    "    legend,\n",
    "    loglog,\n",
    "    semilogx,\n",
    "    show,\n",
    "    subplot,\n",
    "    title,\n",
    "    xlabel,\n",
    "    ylabel,\n",
    ")\n",
    "from scipy.io import loadmat\n",
    "from sklearn import model_selection\n",
    "\n",
    "from dtuimldmtools import rlr_validate\n",
    "import numpy as np\n",
    "import scipy.linalg as linalg\n",
    "from dtuimldmtools import similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import svd\n",
    "from pandas.plotting import scatter_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.linear_model as lm\n",
    "from matplotlib.pyplot import figure, legend, plot, show, xlabel, ylabel\n",
    "# exercise 8.1.1\n",
    "from dtuimldmtools import dbplotf, train_neural_net, visualize_decision_boundary\n",
    "import importlib_resources\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib.pyplot import figure, show, title\n",
    "from scipy.io import loadmat\n",
    "import importlib_resources\n",
    "import numpy as np\n",
    "import sklearn.linear_model as lm\n",
    "from matplotlib.pylab import (\n",
    "    figure,\n",
    "    grid,\n",
    "    legend,\n",
    "    loglog,\n",
    "    semilogx,\n",
    "    show,\n",
    "    subplot,\n",
    "    title,\n",
    "    xlabel,\n",
    "    ylabel,\n",
    ")\n",
    "from scipy.io import loadmat\n",
    "from sklearn import model_selection\n",
    "\n",
    "from dtuimldmtools import rlr_validate\n",
    "\n",
    "# fetch dataset \n",
    "# wine = fetch_ucirepo(id=109) \n",
    "  \n",
    "# # data (as pandas dataframes) \n",
    "# X = wine.data.features \n",
    "# y = wine.data.targets \n",
    "\n",
    "# totaldata= (wine.data)\n",
    "  \n",
    "# metadata \n",
    "# print(wine.metadata) \n",
    "  \n",
    "# variable information \n",
    "# print(wine.variables) \n",
    "\n",
    "# OFFLINE LOADING OF DATA\n",
    "X = np.loadtxt('../wine/wine.data', delimiter=',')\n",
    "\n",
    "#Extract classes from X\n",
    "y = X[:,0]\n",
    "X = np.delete(X,0,axis=1)\n",
    "# y = np.loadtxt('../wine/wine.names', delimiter=',')\n",
    "\n",
    "Xorig = X\n",
    "# Standardizing the data\n",
    "\n",
    "X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "attributeNames = [\n",
    "    \"Alcohol\",\n",
    "    \"Malic acid\",\n",
    "    \"Ash\",\n",
    "    \"Alcalinity of ash\",\n",
    "    \"Magnesium\",\n",
    "    \"Total phenols\",\n",
    "    \"Flavanoids\",\n",
    "    \"Nonflavanoid phenols\",\n",
    "    \"Proanthocyanins\",\n",
    "    \"Color intensity\",\n",
    "    \"Hue\",\n",
    "    \"OD280/OD315 of diluted wines\",\n",
    "    \"Proline\"\n",
    "]\n",
    "\n",
    "N, M = X.shape\n",
    "\n",
    "# Add offset attribute\n",
    "attributeNames = [\"Offset\"] + attributeNames\n",
    "# M = M + 1\n",
    "\n",
    "\n",
    "# Convert class labels to 0, 1, 2\n",
    "y = y - 1\n",
    "\n",
    "classNames = [\"1\", \"2\", \"3\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer Fold:  1\n",
      "Number of hidden units:  1\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.8102478\t5.722911e-05\n",
      "\t\t2000\t0.77741617\t3.7106995e-05\n",
      "\t\t3000\t0.7543922\t1.9752148e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.7543922\t1.9752148e-05\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lauge/Desktop/ML/MLenv/lib/python3.12/site-packages/dtuimldmtools/models/nn_trainer.py:141: RuntimeWarning: overflow encountered in cast\n",
      "  if loss_value < best_final_loss:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t1000\t0.8730904\t0.00022509911\n",
      "\t\t2000\t0.7585879\t8.265213e-05\n",
      "\t\t3000\t0.71848965\t3.591963e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.71848965\t3.591963e-05\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.8908395\t2.4220253e-05\n",
      "\t\t2000\t0.8761322\t7.448901e-05\n",
      "\t\t3000\t0.8075554\t4.0150342e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.8075554\t4.0150342e-05\n",
      "Number of hidden units:  2\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.8245678\t6.77996e-05\n",
      "\t\t2000\t0.5938711\t0.00014049308\n",
      "\t\t3000\t0.5642925\t1.9329407e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.5642925\t1.9329407e-05\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.6279545\t0.0001843931\n",
      "\t\t2000\t0.57166487\t3.4406272e-05\n",
      "\t\t3000\t0.56045616\t1.0847606e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.56045616\t1.0847606e-05\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.6860005\t0.00023106647\n",
      "\t\t2000\t0.6065466\t5.3946696e-05\n",
      "\t\t3000\t0.58788455\t1.7235723e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.58788455\t1.7235723e-05\n",
      "Outer Fold:  1\n",
      "Number of hidden units:  1\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.805317\t7.193633e-05\n",
      "\t\t2000\t0.7542606\t8.2020124e-05\n",
      "\t\t3000\t0.70224804\t5.406367e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.70224804\t5.406367e-05\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lauge/Desktop/ML/MLenv/lib/python3.12/site-packages/dtuimldmtools/models/nn_trainer.py:141: RuntimeWarning: overflow encountered in cast\n",
      "  if loss_value < best_final_loss:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t1000\t0.9001961\t0.00014538251\n",
      "\t\t2000\t0.77596146\t6.436592e-05\n",
      "\t\t3000\t0.7333832\t4.5348577e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.7333832\t4.5348577e-05\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.8895106\t8.877819e-05\n",
      "\t\t2000\t0.86223686\t7.46576e-06\n",
      "\t\t3000\t0.8566322\t3.6181577e-06\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.8566322\t3.6181577e-06\n",
      "Number of hidden units:  2\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.6133242\t0.00021200819\n",
      "\t\t2000\t0.5655703\t2.3395714e-05\n",
      "\t\t3000\t0.55726624\t8.128819e-06\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.55726624\t8.128819e-06\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.6018061\t0.00013428419\n",
      "\t\t2000\t0.56730086\t2.5845833e-05\n",
      "\t\t3000\t0.55875343\t8.960563e-06\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.55875343\t8.960563e-06\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.6196018\t0.00019889853\n",
      "\t\t2000\t0.57094944\t3.2570388e-05\n",
      "\t\t3000\t0.56016296\t1.0853284e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.56016296\t1.0853284e-05\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.61805135\t0.00019689095\n",
      "\t\t2000\t0.5687199\t3.0916493e-05\n",
      "\t\t3000\t0.5587498\t9.814005e-06\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.5587498\t9.814005e-06\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.6855663\t0.00036250448\n",
      "\t\t2000\t0.58227164\t5.8037986e-05\n",
      "\t\t3000\t0.56386805\t1.7018494e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.56386805\t1.7018494e-05\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.6299874\t0.00021160324\n",
      "\t\t2000\t0.5659126\t2.95954e-05\n",
      "\t\t3000\t0.5573861\t7.913202e-06\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.5573861\t7.913202e-06\n",
      "Number of miss-classifications for ANN:\n",
      "\t 0 out of 0\n",
      "Best hidden unit:  2\n",
      "final ANN error for k=1 is 0.02247191011235955\n",
      "the best lambda is:  1.0\n",
      "final logreg error for k=1 is 0.011235955056179775\n",
      "Outer Fold:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lauge/Desktop/ML/MLenv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hidden units:  1\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.85471666\t0.000107103246\n",
      "\t\t2000\t0.78734404\t5.5638924e-05\n",
      "\t\t3000\t0.75834924\t2.5307878e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.75834924\t2.5307878e-05\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lauge/Desktop/ML/MLenv/lib/python3.12/site-packages/dtuimldmtools/models/nn_trainer.py:141: RuntimeWarning: overflow encountered in cast\n",
      "  if loss_value < best_final_loss:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t1000\t0.84059805\t6.048038e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1982\t0.81268656\t2.93371e-07\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.889577\t4.1540356e-05\n",
      "\t\t2000\t0.850976\t1.7510365e-05\n",
      "\t\t3000\t0.8079183\t2.397649e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.8079183\t2.397649e-05\n",
      "Number of hidden units:  2\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.81461585\t0.00014207406\n",
      "\t\t2000\t0.6274368\t0.00013373795\n",
      "\t\t3000\t0.6044309\t1.1340348e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.6044309\t1.1340348e-05\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.6380181\t0.00027598452\n",
      "\t\t2000\t0.58827597\t2.2999313e-05\n",
      "\t\t3000\t0.58046454\t7.495904e-06\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.58046454\t7.495904e-06\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.64580786\t0.00016333495\n",
      "\t\t2000\t0.5965367\t3.5069923e-05\n",
      "\t\t3000\t0.58417314\t1.193766e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.58417314\t1.193766e-05\n",
      "Outer Fold:  2\n",
      "Number of hidden units:  1\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.75080144\t6.477643e-05\n",
      "\t\t2000\t0.72687614\t1.7711924e-05\n",
      "\t\t3000\t0.69222933\t0.00010099137\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.69222933\t0.00010099137\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lauge/Desktop/ML/MLenv/lib/python3.12/site-packages/dtuimldmtools/models/nn_trainer.py:141: RuntimeWarning: overflow encountered in cast\n",
      "  if loss_value < best_final_loss:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t1000\t0.7722469\t9.315169e-05\n",
      "\t\t2000\t0.74115413\t1.5923182e-05\n",
      "\t\t3000\t0.7344081\t5.0319018e-06\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.7344081\t5.0319018e-06\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.759604\t7.2656134e-05\n",
      "\t\t2000\t0.7318719\t2.3617436e-05\n",
      "\t\t3000\t0.6775463\t0.00012077003\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.6775463\t0.00012077003\n",
      "Number of hidden units:  2\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.69913906\t0.0011701942\n",
      "\t\t2000\t0.5675323\t2.8460758e-05\n",
      "\t\t3000\t0.558467\t8.858432e-06\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.558467\t8.858432e-06\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.6516074\t0.00015740067\n",
      "\t\t2000\t0.57700497\t5.12342e-05\n",
      "\t\t3000\t0.5612145\t1.41252685e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.5612145\t1.41252685e-05\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.601477\t0.00019181524\n",
      "\t\t2000\t0.5645633\t2.3331873e-05\n",
      "\t\t3000\t0.5571114\t7.489156e-06\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.5571114\t7.489156e-06\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.6326692\t0.00021560337\n",
      "\t\t2000\t0.57260126\t3.8721657e-05\n",
      "\t\t3000\t0.5601756\t1.2023451e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.5601756\t1.2023451e-05\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.63922536\t0.00025924743\n",
      "\t\t2000\t0.5735248\t3.8971066e-05\n",
      "\t\t3000\t0.560887\t1.2539532e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.560887\t1.2539532e-05\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.6437233\t0.00018570822\n",
      "\t\t2000\t0.57750326\t4.3862718e-05\n",
      "\t\t3000\t0.56291014\t1.4718022e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.56291014\t1.4718022e-05\n",
      "Number of miss-classifications for ANN:\n",
      "\t 0 out of 0\n",
      "Best hidden unit:  2\n",
      "final ANN error for k=2 is 0.02247191011235955\n",
      "the best lambda is:  1.0\n",
      "final logreg error for k=2 is 0.033707865168539325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lauge/Desktop/ML/MLenv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# %% Model fitting and prediction using logistic regression\n",
    "\n",
    "## Crossvalidation\n",
    "# Create OUTER crossvalidation partition for evaluation\n",
    "K = 2\n",
    "CV = model_selection.KFold(K, shuffle=True, random_state=42)\n",
    "# CV = model_selection.KFold(K, shuffle=False)\n",
    "\n",
    "# Values of lambda\n",
    "lambdas = np.logspace(0, 3, 100)\n",
    "# Initialize variables\n",
    "# T = len(lambdas)\n",
    "Error_train = np.empty((K, 1))\n",
    "Error_test = np.empty((K, 1))\n",
    "Error_train_rlr = np.empty((K, 1))\n",
    "Error_test_rlr = np.empty((K, 1))\n",
    "Error_train_nofeatures = np.empty((K, 1))\n",
    "Error_test_nofeatures = np.empty((K, 1))\n",
    "w_rlr = np.empty((M, K))\n",
    "mu = np.empty((K, M - 1))\n",
    "sigma = np.empty((K, M - 1))\n",
    "w_noreg = np.empty((M, K))\n",
    "\n",
    "k = 0\n",
    "for train_index, test_index in CV.split(X, y):\n",
    "    k += 1\n",
    "    Error_train_avg = []\n",
    "    Error_test_avg = []\n",
    "    # extract training and test set for current CV fold\n",
    "    X_train = X[train_index]\n",
    "    y_train = y[train_index]\n",
    "    X_test = X[test_index]\n",
    "    y_test = y[test_index]\n",
    "    internal_cross_validation = 2\n",
    "    \n",
    "    internal_cv = model_selection.KFold(internal_cross_validation, shuffle=True, random_state=42)\n",
    "\n",
    "    # Inner cross-validation to find the best regularization parameter (lambda)\n",
    "    best_lambda = 0.0\n",
    "    min_error = float('inf')\n",
    "    for train_inner_idx, val_inner_idx in internal_cv.split(X_train, y_train):\n",
    "        print(\"Outer Fold: \", k)\n",
    "        #FOR LOGISTIC REGRESSION\n",
    "        for l in lambdas:\n",
    "            Error_train_rlr_fold = []\n",
    "            Error_test_rlr_fold = []\n",
    "            error_inner = []\n",
    "        # For each lambda we do the training with internal cross validation\n",
    "            X_train_inner, y_train_inner = X_train[train_inner_idx], y_train[train_inner_idx]\n",
    "            \n",
    "            X_val_inner, y_val_inner = X_train[val_inner_idx], y_train[val_inner_idx]\n",
    "\n",
    "            logreg = lm.LogisticRegression(C=1/l, solver=\"lbfgs\", tol=1e-4, random_state=1)\n",
    "            logreg.fit(X_train_inner, y_train_inner)\n",
    "            \n",
    "            error_val = np.mean(logreg.predict(X_val_inner) != y_val_inner)\n",
    "\n",
    "        # If the error is less than the minimum error, we update the best lambda\n",
    "            # if error_val < min_error:\n",
    "            #     min_error = error_val\n",
    "            #     best_lambda = l\n",
    "                \n",
    "            error_train_new = np.mean(logreg.predict(X_train_inner) != y_train_inner)\n",
    "            Error_train_rlr_fold.append(error_train_new)\n",
    "            Error_test_rlr_fold.append(error_val)\n",
    "            \n",
    "        #_____________________________________________________\n",
    "        # FOR ANN\n",
    "        # Define the model structure\n",
    "        max_hidden_units = 3\n",
    "        error_log = []\n",
    "        for n_hidden_units in range(1, max_hidden_units):\n",
    "            print(\"Number of hidden units: \", n_hidden_units)\n",
    "            # number of hidden units in the signle hidden layer\n",
    "            C = 3\n",
    "            model = lambda: torch.nn.Sequential(\n",
    "                torch.nn.Linear(M, n_hidden_units),  # M features to H hiden units\n",
    "                torch.nn.ReLU(),  # 1st transfer function\n",
    "                # Output layer:\n",
    "                # H hidden units to C classes\n",
    "                # the nodes and their activation before the transfer\n",
    "                # function is often referred to as logits/logit output\n",
    "                torch.nn.Linear(n_hidden_units, C),  # C logits\n",
    "                # To obtain normalised \"probabilities\" of each class\n",
    "                # we use the softmax-funtion along the \"class\" dimension\n",
    "                # (i.e. not the dimension describing observations)\n",
    "                torch.nn.Softmax(dim=1),  # final tranfer function, normalisation of logit output\n",
    "            )\n",
    "            # Since we're training a multiclass problem, we cannot use binary cross entropy,\n",
    "            # but instead use the general cross entropy loss:\n",
    "            loss_fn = torch.nn.CrossEntropyLoss()\n",
    "            # Train the network:\n",
    "            # C = 3\n",
    "            net, _, _ = train_neural_net(\n",
    "                model,\n",
    "                loss_fn,\n",
    "                X=torch.tensor(X_train_inner, dtype=torch.float),\n",
    "                y=torch.tensor(y_train_inner, dtype=torch.long),\n",
    "                n_replicates=3,\n",
    "                max_iter=3000\n",
    "            )\n",
    "            # Determine probability of each class using trained network\n",
    "            softmax_logits = net(torch.tensor(X_val_inner, dtype=torch.float))\n",
    "            # Get the estimated class as the class with highest probability (argmax on softmax_logits)\n",
    "            y_test_est = (torch.max(softmax_logits, dim=1)[1]).data.numpy()\n",
    "            # Determine errors\n",
    "            e = y_test_est != y_val_inner\n",
    "            # print(\n",
    "            #     \"Number of miss-classifications for ANN:\\n\\t {0} out of {1}\".format(sum(e), len(e))\n",
    "            # )\n",
    "            error_log.append(np.mean(e))\n",
    "\n",
    "            predict = lambda x: (\n",
    "                torch.max(net(torch.tensor(x, dtype=torch.float)), dim=1)[1]\n",
    "            ).data.numpy()\n",
    "        #Find the best amount of hidden units\n",
    "        # print(error_log)\n",
    "        # print(range(1, max_hidden_units))\n",
    "        # print(np.argmin(error_log))\n",
    "        best_hidden_unit = range(1, max_hidden_units)[np.argmin(error_log)]\n",
    "    \n",
    "    #For ANN We just train with the best \n",
    "    #_____________________________________________________\n",
    "    # Average errors across folds for each hidden unit\n",
    "    final_error_for_ANN = []\n",
    "    C = 3\n",
    "    model = lambda: torch.nn.Sequential(\n",
    "        torch.nn.Linear(M, best_hidden_unit),  # M features to H hiden units\n",
    "        torch.nn.ReLU(),  # 1st transfer function\n",
    "        # Output layer:\n",
    "        # H hidden units to C classes\n",
    "        # the nodes and their activation before the transfer\n",
    "        # function is often referred to as logits/logit output\n",
    "        torch.nn.Linear(best_hidden_unit, C),  # C logits\n",
    "        # To obtain normalised \"probabilities\" of each class\n",
    "        # we use the softmax-funtion along the \"class\" dimension\n",
    "        # (i.e. not the dimension describing observations)\n",
    "        torch.nn.Softmax(dim=1),  # final tranfer function, normalisation of logit output\n",
    "    )\n",
    "    # Since we're training a multiclass problem, we cannot use binary cross entropy,\n",
    "    # but instead use the general cross entropy loss:\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    # Train the network:\n",
    "    # C = 3\n",
    "    net, _, _ = train_neural_net(\n",
    "        model,\n",
    "        loss_fn,\n",
    "        X=torch.tensor(X_train, dtype=torch.float),\n",
    "        y=torch.tensor(y_train, dtype=torch.long),\n",
    "        n_replicates=3,\n",
    "        max_iter=3000\n",
    "    )\n",
    "    # Determine probability of each class using trained network\n",
    "    softmax_logits = net(torch.tensor(X_test, dtype=torch.float))\n",
    "    # Get the estimated class as the class with highest probability (argmax on softmax_logits)\n",
    "    y_test_est = (torch.max(softmax_logits, dim=1)[1]).data.numpy()\n",
    "    # Determine errors\n",
    "    print(\n",
    "        \"Number of miss-classifications for ANN:\\n\\t {0} out of {1}\".format(sum(final_error_for_ANN), len(final_error_for_ANN))\n",
    "    )\n",
    "    \n",
    "    predict = lambda x: (\n",
    "        torch.max(net(torch.tensor(x, dtype=torch.float)), dim=1)[1]\n",
    "    ).data.numpy()\n",
    "    \n",
    "    final_error_for_ANN.append( np.mean(y_test_est != y_test))\n",
    "    print(\"Best hidden unit: \", best_hidden_unit)\n",
    "    print(\"final ANN error for k=\" + str(k) + \" is \" + str(np.mean(y_test_est != y_test)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    #FOR LAMBDA / LOGISTIC REGRESSION\n",
    "    #_____________________________________________________\n",
    "    # Average errors across folds for each lambda\n",
    "    Error_test_avg.append(np.mean(Error_test_rlr_fold))\n",
    "    \n",
    "    best_lambda = lambdas[np.argmin(Error_test_avg)]\n",
    "    \n",
    "    \n",
    "    print(\"the best lambda is: \", best_lambda)\n",
    "    \n",
    "    \n",
    "    # Train with best lambda on outer fold and evaluate amount of errors\n",
    "    logreg_best = lm.LogisticRegression(C=best_lambda, solver=\"lbfgs\",  tol=1e-4, random_state=1)\n",
    "    logreg_best.fit(X_train, y_train)\n",
    "\n",
    "    # Multinomial logistic regression\n",
    "    logreg = lm.LogisticRegression(\n",
    "        C=best_lambda,solver=\"lbfgs\", multi_class=\"multinomial\", tol=1e-4, random_state=1\n",
    "    )\n",
    "    logreg.fit(X_train, y_train)\n",
    "    final_error_for_logreg = []\n",
    "    final_error_for_logreg.append(np.mean(logreg.predict(X_test) != y_test))\n",
    "    print(\"final logreg error for k=\" + str(k) + \" is \" + str(np.mean(logreg.predict(X_test) != y_test)))\n",
    "\n",
    "    #_____________________________________________________\n",
    "    \n",
    "    # To display coefficients use print(logreg.coef_). For a 4 class problem with a\n",
    "    # feature space, these weights will have shape (4, 2).\n",
    "\n",
    "    # Number of miss-classifications\n",
    "    # print(\n",
    "    #     \"Number of miss-classifications for Multinormal regression:\\n\\t {0} out of {1}\".format(\n",
    "    #         np.sum(logreg.predict(X_test) != y_test), len(y_test)\n",
    "    #     )\n",
    "    # )\n",
    "    # print(logreg.predict(X_test))\n",
    "    # print(y_test)\n",
    "    \n",
    "    # print(\"Coefficients\")\n",
    "    # print(logreg.coef_)\n",
    "    # # Plotting should be done after the outer loop\n",
    "    # plt.figure(figsize=(10, 6))\n",
    "    # plt.semilogx(lambdas, Error_train_avg, 'b-', label='Training Error')\n",
    "    # plt.semilogx(lambdas, Error_test_avg, 'r-', label='Test Error')\n",
    "    # plt.xlabel('Lambda')\n",
    "    # plt.ylabel('Error')\n",
    "    # plt.legend()\n",
    "    # plt.title('Training vs Test Error for Different Lambda Values')\n",
    "    # plt.grid()\n",
    "    # plt.show()\n",
    "    # k += 1\n",
    "# print(\"final_error_for_ANN: \", (final_error_for_ANN))\n",
    "# print(\"final_error_for_logreg: \", (final_error_for_logreg))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
