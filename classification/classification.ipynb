{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg as linalg\n",
    "from dtuimldmtools import similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import svd\n",
    "from pandas.plotting import scatter_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.linear_model as lm\n",
    "from matplotlib.pyplot import figure, legend, plot, show, xlabel, ylabel\n",
    "# exercise 8.1.1\n",
    "from dtuimldmtools import dbplotf, train_neural_net, visualize_decision_boundary\n",
    "\n",
    "import importlib_resources\n",
    "import numpy as np\n",
    "import sklearn.linear_model as lm\n",
    "from matplotlib.pylab import (\n",
    "    figure,\n",
    "    grid,\n",
    "    legend,\n",
    "    loglog,\n",
    "    semilogx,\n",
    "    show,\n",
    "    subplot,\n",
    "    title,\n",
    "    xlabel,\n",
    "    ylabel,\n",
    ")\n",
    "from scipy.io import loadmat\n",
    "from sklearn import model_selection\n",
    "\n",
    "from dtuimldmtools import rlr_validate\n",
    "import numpy as np\n",
    "import scipy.linalg as linalg\n",
    "from dtuimldmtools import similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import svd\n",
    "from pandas.plotting import scatter_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.linear_model as lm\n",
    "from matplotlib.pyplot import figure, legend, plot, show, xlabel, ylabel\n",
    "# exercise 8.1.1\n",
    "from dtuimldmtools import dbplotf, train_neural_net, visualize_decision_boundary\n",
    "import importlib_resources\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib.pyplot import figure, show, title\n",
    "from scipy.io import loadmat\n",
    "import importlib_resources\n",
    "import numpy as np\n",
    "import sklearn.linear_model as lm\n",
    "from matplotlib.pylab import (\n",
    "    figure,\n",
    "    grid,\n",
    "    legend,\n",
    "    loglog,\n",
    "    semilogx,\n",
    "    show,\n",
    "    subplot,\n",
    "    title,\n",
    "    xlabel,\n",
    "    ylabel,\n",
    ")\n",
    "from scipy.io import loadmat\n",
    "from sklearn import model_selection\n",
    "\n",
    "from dtuimldmtools import rlr_validate\n",
    "\n",
    "# fetch dataset \n",
    "# wine = fetch_ucirepo(id=109) \n",
    "  \n",
    "# # data (as pandas dataframes) \n",
    "# X = wine.data.features \n",
    "# y = wine.data.targets \n",
    "\n",
    "# totaldata= (wine.data)\n",
    "  \n",
    "# metadata \n",
    "# print(wine.metadata) \n",
    "  \n",
    "# variable information \n",
    "# print(wine.variables) \n",
    "\n",
    "# OFFLINE LOADING OF DATA\n",
    "X = np.loadtxt('../wine/wine.data', delimiter=',')\n",
    "\n",
    "#Extract classes from X\n",
    "y = X[:,0]\n",
    "X = np.delete(X,0,axis=1)\n",
    "# y = np.loadtxt('../wine/wine.names', delimiter=',')\n",
    "\n",
    "Xorig = X\n",
    "# Standardizing the data\n",
    "\n",
    "X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "attributeNames = [\n",
    "    \"Alcohol\",\n",
    "    \"Malic acid\",\n",
    "    \"Ash\",\n",
    "    \"Alcalinity of ash\",\n",
    "    \"Magnesium\",\n",
    "    \"Total phenols\",\n",
    "    \"Flavanoids\",\n",
    "    \"Nonflavanoid phenols\",\n",
    "    \"Proanthocyanins\",\n",
    "    \"Color intensity\",\n",
    "    \"Hue\",\n",
    "    \"OD280/OD315 of diluted wines\",\n",
    "    \"Proline\"\n",
    "]\n",
    "\n",
    "N, M = X.shape\n",
    "\n",
    "# Add offset attribute\n",
    "attributeNames = [\"Offset\"] + attributeNames\n",
    "# M = M + 1\n",
    "\n",
    "\n",
    "# Convert class labels to 0, 1, 2\n",
    "y = y - 1\n",
    "\n",
    "classNames = [\"1\", \"2\", \"3\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.8610892\t5.2258412e-05\n",
      "\t\t2000\t0.8036066\t8.098863e-05\n",
      "\t\t3000\t0.7418473\t4.4188484e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.7418473\t4.4188484e-05\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lauge/Desktop/ML/MLenv/lib/python3.12/site-packages/dtuimldmtools/models/nn_trainer.py:141: RuntimeWarning: overflow encountered in cast\n",
      "  if loss_value < best_final_loss:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t1000\t0.8471376\t8.1611004e-05\n",
      "\t\t2000\t0.8091653\t3.1305324e-05\n",
      "\t\t3000\t0.77656454\t6.209035e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.77656454\t6.209035e-05\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.83073884\t5.165658e-05\n",
      "\t\t2000\t0.7875844\t6.326475e-05\n",
      "\t\t3000\t0.72050035\t5.8649817e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.72050035\t5.8649817e-05\n",
      "Number of miss-classifications for ANN:\n",
      "\t 0 out of 18\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.6370824\t0.00019578017\n",
      "\t\t2000\t0.57613933\t4.3863107e-05\n",
      "\t\t3000\t0.56174624\t1.40057955e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.56174624\t1.40057955e-05\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.6334299\t0.00020114159\n",
      "\t\t2000\t0.5776503\t4.3542026e-05\n",
      "\t\t3000\t0.562449\t1.515397e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.562449\t1.515397e-05\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.62938833\t0.00018103838\n",
      "\t\t2000\t0.57801235\t3.588448e-05\n",
      "\t\t3000\t0.56598496\t1.19000415e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.56598496\t1.19000415e-05\n",
      "Number of miss-classifications for ANN:\n",
      "\t 1 out of 18\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.6163238\t0.00015896588\n",
      "\t\t2000\t0.57109636\t3.277073e-05\n",
      "\t\t3000\t0.5601398\t1.09601415e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.5601398\t1.09601415e-05\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.6105646\t0.00013421247\n",
      "\t\t2000\t0.5701632\t3.3869692e-05\n",
      "\t\t3000\t0.5592593\t1.065767e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.5592593\t1.065767e-05\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.56959236\t5.8911322e-05\n",
      "\t\t2000\t0.5561632\t9.430972e-06\n",
      "\t\t3000\t0.55317026\t2.9092682e-06\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.55317026\t2.9092682e-06\n",
      "Number of miss-classifications for ANN:\n",
      "\t 1 out of 18\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.572586\t0.0001275029\n",
      "\t\t2000\t0.5549116\t8.807777e-06\n",
      "\t\t3000\t0.5525518\t1.8338137e-06\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.5525518\t1.8338137e-06\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.61471826\t0.00015230496\n",
      "\t\t2000\t0.5712846\t3.213397e-05\n",
      "\t\t3000\t0.56036174\t1.1274898e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.56036174\t1.1274898e-05\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.56949365\t4.636335e-05\n",
      "\t\t2000\t0.5542245\t8.38852e-06\n",
      "\t\t3000\t0.55219746\t1.5111691e-06\n",
      "\t\tFinal loss:\n",
      "\t\t3000\t0.55219746\t1.5111691e-06\n",
      "Number of miss-classifications for ANN:\n",
      "\t 0 out of 18\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "attempt to get argmin of an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[106], line 114\u001b[0m\n\u001b[1;32m    110\u001b[0m         predict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: (\n\u001b[1;32m    111\u001b[0m             torch\u001b[38;5;241m.\u001b[39mmax(net(torch\u001b[38;5;241m.\u001b[39mtensor(x, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    112\u001b[0m         )\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;66;03m#Find the best amount of hidden units\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m     best_hidden_units \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, max_hidden_units)[\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror_log\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m#FOR LAMBDA / LOGISTIC REGRESSION\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m#_____________________________________________________\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Average errors across folds for each lambda\u001b[39;00m\n\u001b[1;32m    124\u001b[0m Error_test_avg\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mmean(Error_test_rlr_fold))\n",
      "File \u001b[0;32m~/Desktop/ML/MLenv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:1457\u001b[0m, in \u001b[0;36margmin\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m   1368\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;124;03mReturns the indices of the minimum values along an axis.\u001b[39;00m\n\u001b[1;32m   1370\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1454\u001b[0m \u001b[38;5;124;03m(2, 1, 4)\u001b[39;00m\n\u001b[1;32m   1455\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1456\u001b[0m kwds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeepdims\u001b[39m\u001b[38;5;124m'\u001b[39m: keepdims} \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m-> 1457\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43margmin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ML/MLenv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:54\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     52\u001b[0m bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, method, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bound \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[0;32m~/Desktop/ML/MLenv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:46\u001b[0m, in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# As this already tried the method, subok is maybe quite reasonable here\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# but this follows what was done before. TODO: revisit this.\u001b[39;00m\n\u001b[1;32m     45\u001b[0m arr, \u001b[38;5;241m=\u001b[39m conv\u001b[38;5;241m.\u001b[39mas_arrays(subok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 46\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m conv\u001b[38;5;241m.\u001b[39mwrap(result, to_scalar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mValueError\u001b[0m: attempt to get argmin of an empty sequence"
     ]
    }
   ],
   "source": [
    "# %% Model fitting and prediction using logistic regression\n",
    "\n",
    "## Crossvalidation\n",
    "# Create OUTER crossvalidation partition for evaluation\n",
    "K = 10\n",
    "CV = model_selection.KFold(K, shuffle=True, random_state=42)\n",
    "# CV = model_selection.KFold(K, shuffle=False)\n",
    "\n",
    "# Values of lambda\n",
    "lambdas = np.logspace(0, 3, 100)\n",
    "# Initialize variables\n",
    "# T = len(lambdas)\n",
    "Error_train = np.empty((K, 1))\n",
    "Error_test = np.empty((K, 1))\n",
    "Error_train_rlr = np.empty((K, 1))\n",
    "Error_test_rlr = np.empty((K, 1))\n",
    "Error_train_nofeatures = np.empty((K, 1))\n",
    "Error_test_nofeatures = np.empty((K, 1))\n",
    "w_rlr = np.empty((M, K))\n",
    "mu = np.empty((K, M - 1))\n",
    "sigma = np.empty((K, M - 1))\n",
    "w_noreg = np.empty((M, K))\n",
    "\n",
    "k = 0\n",
    "for train_index, test_index in CV.split(X, y):\n",
    "    Error_train_avg = []\n",
    "    Error_test_avg = []\n",
    "    # extract training and test set for current CV fold\n",
    "    X_train = X[train_index]\n",
    "    y_train = y[train_index]\n",
    "    X_test = X[test_index]\n",
    "    y_test = y[test_index]\n",
    "    internal_cross_validation = 10\n",
    "    \n",
    "    internal_cv = model_selection.KFold(internal_cross_validation, shuffle=True, random_state=42)\n",
    "\n",
    "    # Inner cross-validation to find the best regularization parameter (lambda)\n",
    "    best_lambda = 0.0\n",
    "    min_error = float('inf')\n",
    "    for train_inner_idx, val_inner_idx in internal_cv.split(X_train, y_train):\n",
    "        \n",
    "        #FOR LOGISTIC REGRESSION\n",
    "        for l in lambdas:\n",
    "            Error_train_rlr_fold = []\n",
    "            Error_test_rlr_fold = []\n",
    "            error_inner = []\n",
    "        # For each lambda we do the training with internal cross validation\n",
    "            X_train_inner, y_train_inner = X_train[train_inner_idx], y_train[train_inner_idx]\n",
    "            \n",
    "            X_val_inner, y_val_inner = X_train[val_inner_idx], y_train[val_inner_idx]\n",
    "\n",
    "            logreg = lm.LogisticRegression(C=1/l, solver=\"lbfgs\", tol=1e-4, random_state=1)\n",
    "            logreg.fit(X_train_inner, y_train_inner)\n",
    "            \n",
    "            error_val = np.mean(logreg.predict(X_val_inner) != y_val_inner)\n",
    "\n",
    "        # If the error is less than the minimum error, we update the best lambda\n",
    "            # if error_val < min_error:\n",
    "            #     min_error = error_val\n",
    "            #     best_lambda = l\n",
    "                \n",
    "            error_train_new = np.mean(logreg.predict(X_train_inner) != y_train_inner)\n",
    "            Error_train_rlr_fold.append(error_train_new)\n",
    "            Error_test_rlr_fold.append(error_val)\n",
    "            \n",
    "        #_____________________________________________________\n",
    "        # FOR ANN\n",
    "        # Define the model structure\n",
    "        max_hidden_units = 5\n",
    "        error_log = []\n",
    "        for n_hidden_units in range(1, max_hidden_units):\n",
    "            # number of hidden units in the signle hidden layer\n",
    "            C = 3\n",
    "            model = lambda: torch.nn.Sequential(\n",
    "                torch.nn.Linear(M, n_hidden_units),  # M features to H hiden units\n",
    "                torch.nn.ReLU(),  # 1st transfer function\n",
    "                # Output layer:\n",
    "                # H hidden units to C classes\n",
    "                # the nodes and their activation before the transfer\n",
    "                # function is often referred to as logits/logit output\n",
    "                torch.nn.Linear(n_hidden_units, C),  # C logits\n",
    "                # To obtain normalised \"probabilities\" of each class\n",
    "                # we use the softmax-funtion along the \"class\" dimension\n",
    "                # (i.e. not the dimension describing observations)\n",
    "                torch.nn.Softmax(dim=1),  # final tranfer function, normalisation of logit output\n",
    "            )\n",
    "            # Since we're training a multiclass problem, we cannot use binary cross entropy,\n",
    "            # but instead use the general cross entropy loss:\n",
    "            loss_fn = torch.nn.CrossEntropyLoss()\n",
    "            # Train the network:\n",
    "            # C = 3\n",
    "            net, _, _ = train_neural_net(\n",
    "                model,\n",
    "                loss_fn,\n",
    "                X=torch.tensor(X_train, dtype=torch.float),\n",
    "                y=torch.tensor(y_train, dtype=torch.long),\n",
    "                n_replicates=3,\n",
    "                max_iter=3000\n",
    "            )\n",
    "            # Determine probability of each class using trained network\n",
    "            softmax_logits = net(torch.tensor(X_test, dtype=torch.float))\n",
    "            # Get the estimated class as the class with highest probability (argmax on softmax_logits)\n",
    "            y_test_est = (torch.max(softmax_logits, dim=1)[1]).data.numpy()\n",
    "            # Determine errors\n",
    "            e = y_test_est != y_test\n",
    "            print(\n",
    "                \"Number of miss-classifications for ANN:\\n\\t {0} out of {1}\".format(sum(e), len(e))\n",
    "            )\n",
    "\n",
    "            predict = lambda x: (\n",
    "                torch.max(net(torch.tensor(x, dtype=torch.float)), dim=1)[1]\n",
    "            ).data.numpy()\n",
    "        #Find the best amount of hidden units\n",
    "        best_hidden_units = range(1, max_hidden_units)[np.argmin(error_log)]\n",
    "    \n",
    "    #For ANN\n",
    "    #_____________________________________________________\n",
    "    # Average errors across folds for each hidden unit\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #FOR LAMBDA / LOGISTIC REGRESSION\n",
    "    #_____________________________________________________\n",
    "    # Average errors across folds for each lambda\n",
    "    Error_test_avg.append(np.mean(Error_test_rlr_fold))\n",
    "    \n",
    "    best_lambda = lambdas[np.argmin(Error_test_avg)]\n",
    "    \n",
    "    # Train with best lambda on outer fold and evaluate amount of errors\n",
    "    logreg_best = lm.LogisticRegression(C=best_lambda, solver=\"lbfgs\",  tol=1e-4, random_state=1)\n",
    "    logreg_best.fit(X_train, y_train)\n",
    "\n",
    "    # Multinomial logistic regression\n",
    "    logreg = lm.LogisticRegression(\n",
    "        C=best_lambda,solver=\"lbfgs\", multi_class=\"multinomial\", tol=1e-4, random_state=1\n",
    "    )\n",
    "    logreg.fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "    print(\"the best lambda is: \", best_lambda)\n",
    "    \n",
    "    print(lambdas)\n",
    "\n",
    "    #_____________________________________________________\n",
    "    \n",
    "    # To display coefficients use print(logreg.coef_). For a 4 class problem with a\n",
    "    # feature space, these weights will have shape (4, 2).\n",
    "\n",
    "    # Number of miss-classifications\n",
    "    # print(\n",
    "    #     \"Number of miss-classifications for Multinormal regression:\\n\\t {0} out of {1}\".format(\n",
    "    #         np.sum(logreg.predict(X_test) != y_test), len(y_test)\n",
    "    #     )\n",
    "    # )\n",
    "    # print(logreg.predict(X_test))\n",
    "    # print(y_test)\n",
    "    \n",
    "    # print(\"Coefficients\")\n",
    "    # print(logreg.coef_)\n",
    "    # # Plotting should be done after the outer loop\n",
    "    # plt.figure(figsize=(10, 6))\n",
    "    # plt.semilogx(lambdas, Error_train_avg, 'b-', label='Training Error')\n",
    "    # plt.semilogx(lambdas, Error_test_avg, 'r-', label='Test Error')\n",
    "    # plt.xlabel('Lambda')\n",
    "    # plt.ylabel('Error')\n",
    "    # plt.legend()\n",
    "    # plt.title('Training vs Test Error for Different Lambda Values')\n",
    "    # plt.grid()\n",
    "    # plt.show()\n",
    "    # k += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
